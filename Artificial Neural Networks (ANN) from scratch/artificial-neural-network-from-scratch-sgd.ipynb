{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228bbcbb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:53.865579Z",
     "iopub.status.busy": "2021-07-31T18:07:53.865101Z",
     "iopub.status.idle": "2021-07-31T18:07:54.980925Z",
     "shell.execute_reply": "2021-07-31T18:07:54.980442Z",
     "shell.execute_reply.started": "2021-07-31T17:49:56.858454Z"
    },
    "papermill": {
     "duration": 1.151429,
     "end_time": "2021-07-31T18:07:54.981047",
     "exception": false,
     "start_time": "2021-07-31T18:07:53.829618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train_data.csv\n",
      "/kaggle/input/titanic/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e244f",
   "metadata": {
    "papermill": {
     "duration": 0.023606,
     "end_time": "2021-07-31T18:07:55.028947",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.005341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b56b5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.079114Z",
     "iopub.status.busy": "2021-07-31T18:07:55.078606Z",
     "iopub.status.idle": "2021-07-31T18:07:55.117523Z",
     "shell.execute_reply": "2021-07-31T18:07:55.116956Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.266253Z"
    },
    "papermill": {
     "duration": 0.066079,
     "end_time": "2021-07-31T18:07:55.117712",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.051633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Sex     Age      Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0            1         0    1  0.2750  0.014151         0         0         1   \n",
       "1            2         1    0  0.4750  0.139136         1         0         0   \n",
       "2            3         1    0  0.3250  0.015469         0         0         1   \n",
       "3            4         1    0  0.4375  0.103644         1         0         0   \n",
       "4            5         0    1  0.4375  0.015713         0         0         1   \n",
       "\n",
       "   Family_size  Emb_1  Emb_2  Emb_3  \n",
       "0          0.1      0      0      1  \n",
       "1          0.1      1      0      0  \n",
       "2          0.0      0      0      1  \n",
       "3          0.1      0      0      1  \n",
       "4          0.0      0      0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ads = pd.read_csv('../input/titanic/train_data.csv')\n",
    "input_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n",
    "#-----------------------------------------------------------------\n",
    "print(input_ads.shape)\n",
    "input_ads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047f481",
   "metadata": {
    "papermill": {
     "duration": 0.023188,
     "end_time": "2021-07-31T18:07:55.166050",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.142862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23b8582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.227745Z",
     "iopub.status.busy": "2021-07-31T18:07:55.227251Z",
     "iopub.status.idle": "2021-07-31T18:07:55.240301Z",
     "shell.execute_reply": "2021-07-31T18:07:55.239821Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.342199Z"
    },
    "papermill": {
     "duration": 0.045563,
     "end_time": "2021-07-31T18:07:55.240413",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.194850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Sex  Age  Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0            0         0    0    0     0         0         0         0   \n",
       "\n",
       "   Family_size  Emb_1  Emb_2  Emb_3  \n",
       "0            0      0      0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(input_ads.isnull().sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97721e",
   "metadata": {
    "papermill": {
     "duration": 0.025015,
     "end_time": "2021-07-31T18:07:55.291023",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.266008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45031e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.340783Z",
     "iopub.status.busy": "2021-07-31T18:07:55.340317Z",
     "iopub.status.idle": "2021-07-31T18:07:55.378178Z",
     "shell.execute_reply": "2021-07-31T18:07:55.378572Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.362282Z"
    },
    "papermill": {
     "duration": 0.063789,
     "end_time": "2021-07-31T18:07:55.378743",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.314954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>396.500000</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.368244</td>\n",
       "      <td>0.064677</td>\n",
       "      <td>0.243687</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>0.088636</td>\n",
       "      <td>0.185606</td>\n",
       "      <td>0.092172</td>\n",
       "      <td>0.720960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>228.774999</td>\n",
       "      <td>0.487223</td>\n",
       "      <td>0.477980</td>\n",
       "      <td>0.162994</td>\n",
       "      <td>0.100987</td>\n",
       "      <td>0.429577</td>\n",
       "      <td>0.406373</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>0.154485</td>\n",
       "      <td>0.389034</td>\n",
       "      <td>0.289451</td>\n",
       "      <td>0.448811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>396.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>594.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.061045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>792.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived         Sex         Age        Fare  \\\n",
       "count   792.000000  792.000000  792.000000  792.000000  792.000000   \n",
       "mean    396.500000    0.386364    0.647727    0.368244    0.064677   \n",
       "std     228.774999    0.487223    0.477980    0.162994    0.100987   \n",
       "min       1.000000    0.000000    0.000000    0.008375    0.000000   \n",
       "25%     198.750000    0.000000    0.000000    0.275000    0.015469   \n",
       "50%     396.500000    0.000000    1.000000    0.350000    0.028302   \n",
       "75%     594.250000    1.000000    1.000000    0.437500    0.061045   \n",
       "max     792.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         Pclass_1    Pclass_2    Pclass_3  Family_size       Emb_1  \\\n",
       "count  792.000000  792.000000  792.000000   792.000000  792.000000   \n",
       "mean     0.243687    0.208333    0.547980     0.088636    0.185606   \n",
       "std      0.429577    0.406373    0.498007     0.154485    0.389034   \n",
       "min      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000     0.000000    0.000000   \n",
       "75%      0.000000    0.000000    1.000000     0.100000    0.000000   \n",
       "max      1.000000    1.000000    1.000000     1.000000    1.000000   \n",
       "\n",
       "            Emb_2       Emb_3  \n",
       "count  792.000000  792.000000  \n",
       "mean     0.092172    0.720960  \n",
       "std      0.289451    0.448811  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    0.000000  \n",
       "50%      0.000000    1.000000  \n",
       "75%      0.000000    1.000000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ads.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60610470",
   "metadata": {
    "papermill": {
     "duration": 0.024792,
     "end_time": "2021-07-31T18:07:55.428575",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.403783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f6f07a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.481317Z",
     "iopub.status.busy": "2021-07-31T18:07:55.480868Z",
     "iopub.status.idle": "2021-07-31T18:07:55.489053Z",
     "shell.execute_reply": "2021-07-31T18:07:55.488605Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.428245Z"
    },
    "papermill": {
     "duration": 0.035299,
     "end_time": "2021-07-31T18:07:55.489164",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.453865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    486\n",
       "1    306\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total survived vs not-survived split in the training data\n",
    "input_ads['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dad1c",
   "metadata": {
    "papermill": {
     "duration": 0.025064,
     "end_time": "2021-07-31T18:07:55.539517",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.514453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Manipulation of data into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a5e4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.598789Z",
     "iopub.status.busy": "2021-07-31T18:07:55.598162Z",
     "iopub.status.idle": "2021-07-31T18:07:55.620507Z",
     "shell.execute_reply": "2021-07-31T18:07:55.619863Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.441673Z"
    },
    "papermill": {
     "duration": 0.055759,
     "end_time": "2021-07-31T18:07:55.620666",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.564907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train % of total data: 88.78923766816143\n",
      "(792, 11)\n",
      "(100, 11)\n",
      "(792, 1)\n"
     ]
    }
   ],
   "source": [
    "target = 'Survived' #To predict\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Splitting into X & Y datasets (supervised training)\n",
    "X = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\n",
    "y = input_ads[target]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Since test data is already placed in the input folder separately, we will just import it\n",
    "test_ads = pd.read_csv('../input/titanic/test_data.csv')\n",
    "test_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n",
    "\n",
    "#Splitting into X & Y datasets (supervised training)\n",
    "X_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\n",
    "y_test = test_ads[target]\n",
    "\n",
    "print('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n",
    "#--------------------------------------------------------------------------------\n",
    "#Manipulation of datasets for convenience and consistency\n",
    "X_arr = np.array(X)\n",
    "X_test_arr = np.array(X_test)\n",
    "\n",
    "y_arr = np.array(y).reshape(X_arr.shape[0],1)\n",
    "y_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Basic Summary\n",
    "print(X_arr.shape)\n",
    "print(X_test_arr.shape)\n",
    "print(y_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530d024",
   "metadata": {
    "papermill": {
     "duration": 0.026598,
     "end_time": "2021-07-31T18:07:55.678356",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.651758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Standard scaling the x-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a5cc95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.734575Z",
     "iopub.status.busy": "2021-07-31T18:07:55.733707Z",
     "iopub.status.idle": "2021-07-31T18:07:55.739919Z",
     "shell.execute_reply": "2021-07-31T18:07:55.740384Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.473124Z"
    },
    "papermill": {
     "duration": 0.037043,
     "end_time": "2021-07-31T18:07:55.740551",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.703508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.72986525,  0.73746841, -0.57243072, -0.50063632, -0.56762987,\n",
       "        -0.51298918,  0.90823168,  0.07360462, -0.47739604, -0.31863757,\n",
       "         0.62212561],\n",
       "       [-1.72549138, -1.35599029,  0.65538585,  0.73777138,  1.76171137,\n",
       "        -0.51298918, -1.10104065,  0.07360462,  2.0946969 , -0.31863757,\n",
       "        -1.60739242],\n",
       "       [-1.72111752, -1.35599029, -0.26547658, -0.48758178, -0.56762987,\n",
       "        -0.51298918,  0.90823168, -0.57411602, -0.47739604, -0.31863757,\n",
       "         0.62212561]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_arr = scaler.fit_transform(X_arr)\n",
    "X_test_arr = scaler.transform(X_test_arr)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "X_arr[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dcca03",
   "metadata": {
    "papermill": {
     "duration": 0.038134,
     "end_time": "2021-07-31T18:07:55.817800",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.779666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Artificial Neural Network (ANN) from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9089764",
   "metadata": {
    "papermill": {
     "duration": 0.038037,
     "end_time": "2021-07-31T18:07:55.895024",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.856987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDFs for activation, initialization, layer_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af044bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:55.974429Z",
     "iopub.status.busy": "2021-07-31T18:07:55.973841Z",
     "iopub.status.idle": "2021-07-31T18:07:55.981445Z",
     "shell.execute_reply": "2021-07-31T18:07:55.982040Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.485272Z"
    },
    "papermill": {
     "duration": 0.048686,
     "end_time": "2021-07-31T18:07:55.982182",
     "exception": false,
     "start_time": "2021-07-31T18:07:55.933496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All popular activation functions\n",
    "def activation_fn(z,type_):\n",
    "    \n",
    "    #print('Activation : ',type_)\n",
    "    \n",
    "    if type_=='linear':\n",
    "        activated_arr = z\n",
    "    \n",
    "    elif type_=='sigmoid':\n",
    "        activated_arr = 1/(1+np.exp(-z))\n",
    "    \n",
    "    elif type_=='relu': \n",
    "        activated_arr = np.maximum(np.zeros(z.shape),z)\n",
    "    \n",
    "    elif type_=='tanh':\n",
    "        activated_arr = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    \n",
    "    elif type_=='leaky_relu':\n",
    "        activated_arr = np.maximum(0.01*z,z)\n",
    "    \n",
    "    elif type_=='softmax':\n",
    "        exp_ = np.exp(z)\n",
    "        exp_sum = np.sum(exp_)\n",
    "        activated_arr = exp_/exp_sum\n",
    "        \n",
    "    return activated_arr\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Initialization of params\n",
    "def generate_param_grid(a_prev,n_hidden,hidden_size_list):\n",
    "    \n",
    "    parameters = {}\n",
    "    features = a_prev.shape[0] #Total features\n",
    "    n_examples = a_prev.shape[1]\n",
    "    \n",
    "    for n_hidden_idx in range(1,n_hidden+1):\n",
    "        \n",
    "        n_hidden_nodes = hidden_size_list[n_hidden_idx] #Should start from 0\n",
    "        \n",
    "        #print('#------------ Layer :',n_hidden_idx,'---- Size :',n_hidden_nodes,'---- Prev features :',features,'------#')\n",
    "\n",
    "        parameters['w' + str(n_hidden_idx)] = np.random.rand(n_hidden_nodes,features) * 0.1 #Xavier Initialization\n",
    "        parameters['b' + str(n_hidden_idx)] = np.zeros((n_hidden_nodes,1)) * 0.1\n",
    "        \n",
    "        features = n_hidden_nodes\n",
    "    \n",
    "    return parameters#Return randomly initiated params\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "#Propagation between z and activation\n",
    "def layer_propagation(a_prev,w,b,activation):\n",
    "    \n",
    "    #print(a_prev.shape)\n",
    "    #print(w.shape)\n",
    "    #print(b.shape)\n",
    "    \n",
    "    z_ = np.dot(w,a_prev) + b\n",
    "    \n",
    "    a = activation_fn(z=z_,\n",
    "                      type_=activation)\n",
    "    \n",
    "    return z_,a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5af5c",
   "metadata": {
    "papermill": {
     "duration": 0.03828,
     "end_time": "2021-07-31T18:07:56.059260",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.020980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eccc2aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.139535Z",
     "iopub.status.busy": "2021-07-31T18:07:56.138924Z",
     "iopub.status.idle": "2021-07-31T18:07:56.144887Z",
     "shell.execute_reply": "2021-07-31T18:07:56.144300Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.500003Z"
    },
    "papermill": {
     "duration": 0.047155,
     "end_time": "2021-07-31T18:07:56.145038",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.097883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_propagation(params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list):\n",
    "    \n",
    "    cache = {'a0' : data_x.T}\n",
    "    a = data_x.T.copy()\n",
    "    \n",
    "    for layer_idx in range(1,n_hidden+1):\n",
    "        \n",
    "        #print('#---------- Layer :',layer_idx,'-- No of Nodes :',hidden_size_list[layer_idx])\n",
    "        #nodes = hidden_size_list[layer_idx]\n",
    "        activation_ = activation_list[layer_idx]\n",
    "        w_ = params_dict['w'+str(layer_idx)]\n",
    "        b_ = params_dict['b'+str(layer_idx)]\n",
    "        \n",
    "        z,a = layer_propagation(a_prev=a,\n",
    "                                 w=w_,\n",
    "                                 b=b_,\n",
    "                                 activation=activation_)\n",
    "        \n",
    "        cache['z'+str(layer_idx)] = z\n",
    "        cache['a'+str(layer_idx)] = a\n",
    "    \n",
    "    return cache,a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc076ab2",
   "metadata": {
    "papermill": {
     "duration": 0.038795,
     "end_time": "2021-07-31T18:07:56.223142",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.184347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for cost calculation, gradient calculation & back-propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a36e90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.303582Z",
     "iopub.status.busy": "2021-07-31T18:07:56.302942Z",
     "iopub.status.idle": "2021-07-31T18:07:56.315053Z",
     "shell.execute_reply": "2021-07-31T18:07:56.315552Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.518756Z"
    },
    "papermill": {
     "duration": 0.053886,
     "end_time": "2021-07-31T18:07:56.315754",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.261868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculation of the total cost incurred by the model\n",
    "def cost_calculation(activation_list,y_true,y_pred):\n",
    "    \n",
    "    if activation_list[-1]=='sigmoid':\n",
    "        #print('sig')\n",
    "        m = y_true.shape[1]\n",
    "        cost = (-1/m) * np.sum((y_true * np.log(y_pred)) + ((1-y_true) * np.log(1 - y_pred)))\n",
    "        \n",
    "    elif activation_list[-1]=='linear':\n",
    "        \n",
    "        m = y_true.shape[1]\n",
    "        cost = (1/m) * np.sum(np.square(y_true-y_pred))\n",
    "        \n",
    "     ##-------------------->> Softmax to be added <<----------------------\n",
    "    \n",
    "    return cost\n",
    "\n",
    "#Gradient of the activation functions wrt corresponding z\n",
    "#--------------------------------------------------------------------------------------------\n",
    "#Gradient for each activation type\n",
    "def grad_fn_dz(activation,a):\n",
    "    \n",
    "    if activation=='linear':\n",
    "        grad = 1\n",
    "     \n",
    "    elif activation=='sigmoid':\n",
    "        grad = a*(1-a)\n",
    "        \n",
    "    elif activation=='tanh':\n",
    "        grad = np.square(1-a)\n",
    "        \n",
    "    elif activation=='relu':\n",
    "        grad = np.where(a>=0,1,0)\n",
    "    \n",
    "    elif activation=='leaky_relu':\n",
    "        grad = np.where(a>=0,1,0.01)\n",
    "    \n",
    "    ##-------------------->> Softmax to be added <<----------------------\n",
    "    \n",
    "    return grad\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------\n",
    "#UDF for gradient of loss function wrt last layer\n",
    "def dL_last_layer(activation_list,y_true,y_pred):\n",
    "    \n",
    "    if activation_list[-1]=='sigmoid':\n",
    "        \n",
    "        #print('Last Layer y true shape :',y_true.shape)\n",
    "        #print('Last Layer y pred shape :',y_pred.shape)\n",
    "        \n",
    "        grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred)))\n",
    "        #print('Last Layer gradient shape :',grad_final_layer.shape)\n",
    "        \n",
    "    elif activation_list[-1]=='linear':\n",
    "        \n",
    "        grad_final_layer = - 2 * (y_true-y_pred) #Check the sign\n",
    "        \n",
    "    return grad_final_layer\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "#Back=Propagation         \n",
    "def back_propagation(cache,params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list,y_pred):\n",
    "    \n",
    "    grads_cache = {}\n",
    "    #db_cache = {}\n",
    "    \n",
    "    da = dL_last_layer(activation_list=activation_list,\n",
    "                             y_true=data_y.T,\n",
    "                             y_pred=y_pred)\n",
    "    #print('Final da shape :',da.shape)\n",
    "    \n",
    "    m = data_y.shape[0] #Data in the batches\n",
    "    \n",
    "    #print('dm in backprop :',m)\n",
    "    for layer_idx in list(reversed(range(1,n_hidden+1))):\n",
    "        \n",
    "        #print('# -------- Layer :',layer_idx,'-------- Size :',hidden_size_list[layer_idx],'--------#')\n",
    "        \n",
    "        activation_ = activation_list[layer_idx]\n",
    "        a = cache['a'+str(layer_idx)]\n",
    "        a_prev = cache['a'+str(layer_idx-1)]\n",
    "        w = params_dict['w'+str(layer_idx)]\n",
    "        \n",
    "#         print('Shape of a:',a.shape)\n",
    "#         print('Shape of a_prev:',a_prev.shape)\n",
    "#         print('SHape of w:',w.shape)\n",
    "        \n",
    "        #z = \n",
    "        \n",
    "        dz =  da * (grad_fn_dz(activation=activation_,a=a))\n",
    "        \n",
    "        #print('dz shape :',dz.shape)\n",
    "                     \n",
    "        dw = (1/m) * np.dot(dz, a_prev.T)\n",
    "        #print('dw shape :',dw.shape)\n",
    "        grads_cache['dw'+str(layer_idx)] = dw\n",
    "                     \n",
    "        db = (1/m) * np.sum(dz, axis=1,keepdims=True)\n",
    "        #print('db shape :',db.shape)\n",
    "        grads_cache['db'+str(layer_idx)] = db\n",
    "        \n",
    "        da = np.dot(w.T,dz)\n",
    "        #print('da shape :',da.shape)\n",
    "\n",
    "    return grads_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581222cc",
   "metadata": {
    "papermill": {
     "duration": 0.038354,
     "end_time": "2021-07-31T18:07:56.392870",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.354516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for updating weights through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49641187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.471933Z",
     "iopub.status.busy": "2021-07-31T18:07:56.471287Z",
     "iopub.status.idle": "2021-07-31T18:07:56.475746Z",
     "shell.execute_reply": "2021-07-31T18:07:56.476282Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.542060Z"
    },
    "papermill": {
     "duration": 0.045578,
     "end_time": "2021-07-31T18:07:56.476460",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.430882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_weights(params,grads_cache,alpha,n_hidden):\n",
    "    \n",
    "    for layer_idx in list(reversed(range(1,n_hidden+1))):\n",
    "        \n",
    "        #print('#---- layer :',layer_idx,'----#')\n",
    "        \n",
    "        dw = grads_cache['dw'+str(layer_idx)]\n",
    "        db = grads_cache['db'+str(layer_idx)]\n",
    "        \n",
    "#         print('dw shape :',dw.shape)\n",
    "#         print('db shape :',db.shape)\n",
    "#         print('w shape :',params['w'+str(layer_idx)].shape)\n",
    "        \n",
    "        params['w'+str(layer_idx)] -= alpha * dw\n",
    "        params['b'+str(layer_idx)] -= alpha * db\n",
    "\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a81d6",
   "metadata": {
    "papermill": {
     "duration": 0.038313,
     "end_time": "2021-07-31T18:07:56.553804",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.515491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697a4f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.634232Z",
     "iopub.status.busy": "2021-07-31T18:07:56.633506Z",
     "iopub.status.idle": "2021-07-31T18:07:56.637295Z",
     "shell.execute_reply": "2021-07-31T18:07:56.637847Z",
     "shell.execute_reply.started": "2021-07-31T17:49:58.557990Z"
    },
    "papermill": {
     "duration": 0.045681,
     "end_time": "2021-07-31T18:07:56.638023",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.592342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(params,test_x,n_hidden,hidden_size_list,activation_list,threshold):\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    #Forward Propagation on trained weights\n",
    "    cache,y_pred = forward_propagation(params_dict=params,\n",
    "                                  data_x=test_x,\n",
    "                                  data_y=None,\n",
    "                                  n_hidden=n_hidden,\n",
    "                                  hidden_size_list=hidden_size_list,\n",
    "                                  activation_list=activation_list)\n",
    "    #print(cache)\n",
    "    preds = np.where(y_pred>threshold,1,0).astype(float)\n",
    "    return cache,np.round(y_pred,4),preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b936a",
   "metadata": {
    "papermill": {
     "duration": 0.037972,
     "end_time": "2021-07-31T18:07:56.714189",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.676217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stochastic Gradient Descent (SGD) for training of the ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd51f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.794560Z",
     "iopub.status.busy": "2021-07-31T18:07:56.793972Z",
     "iopub.status.idle": "2021-07-31T18:07:56.803057Z",
     "shell.execute_reply": "2021-07-31T18:07:56.803537Z",
     "shell.execute_reply.started": "2021-07-31T17:54:40.542084Z"
    },
    "papermill": {
     "duration": 0.050448,
     "end_time": "2021-07-31T18:07:56.803672",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.753224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ANN_train_sgd(data_x_overall,data_y_overall,batch_size,alpha,n_iters,n_hidden,hidden_size_list,activation_list):\n",
    "    \n",
    "    print('Total training rows :',data_x_overall.shape[0])\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    #Creating x-y batches according to the provided batch_size\n",
    "    \n",
    "    n_batches = data_x_overall.shape[0]//batch_size\n",
    "    print('Total Batches to create in each epoch/iter :',n_batches)\n",
    "    \n",
    "    batches_x = np.array_split(data_x_overall,n_batches)\n",
    "    print('Total Batches of X:',len(batches_x))\n",
    "\n",
    "    batches_y = np.array_split(data_y_overall,n_batches)\n",
    "    print('Total Batches of y:',len(batches_y))\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    cost_history = [] #Record of cost through epochs\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    #Initialization of params\n",
    "    params_dict = generate_param_grid(a_prev=data_x_overall.T,\n",
    "                             n_hidden=n_hidden,\n",
    "                             hidden_size_list=hidden_size_list)\n",
    "    print('#----------------- Initial params ------------------#')\n",
    "    print(params_dict)\n",
    "    initial_params_abcd = params_dict.copy()\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    cache_tray = []\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "\n",
    "        if (epoch>0) & (epoch%100==0):\n",
    "            print('#----------------------------------- Epoch :',epoch,'--------------------------------------#')\n",
    "            print('cost :',cost)\n",
    "            \n",
    "        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n",
    "            \n",
    "            #-------------------------------------------------------------------------\n",
    "            #For each batch of data\n",
    "            data_x = batches_x[j]\n",
    "            data_y = batches_y[j]\n",
    "\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Forward Propagation\n",
    "            cache,y_pred = forward_propagation(params_dict=params_dict,\n",
    "                                          data_x=data_x,\n",
    "                                          data_y=data_y,\n",
    "                                          n_hidden=n_hidden,\n",
    "                                          hidden_size_list=hidden_size_list,\n",
    "                                          activation_list=activation_list)\n",
    "            #print(np.max(y_pred))\n",
    "            #cache_tray.append(cache)\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Cost calculation\n",
    "            cost = cost_calculation(activation_list=activation_list,\n",
    "                             y_true=data_y.T,\n",
    "                             y_pred=y_pred)\n",
    "\n",
    "            #cost_history.append(cost)\n",
    "            #print('cost :',cost)\n",
    "\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Back Propagation\n",
    "            grads_cache_ = back_propagation(cache=cache,\n",
    "                                           params_dict=params_dict,\n",
    "                                           data_x=data_x,\n",
    "                                           data_y=data_y,\n",
    "                                           n_hidden=n_hidden,\n",
    "                                           hidden_size_list=hidden_size_list,\n",
    "                                           activation_list=activation_list,\n",
    "                                           y_pred=y_pred)\n",
    "\n",
    "            #------------------------------------------------------------------------\n",
    "            #Updating weights\n",
    "            params_dict = update_weights(params=params_dict,\n",
    "                                         grads_cache=grads_cache_,\n",
    "                                         alpha=alpha,\n",
    "                                         n_hidden=n_hidden)\n",
    "            \n",
    "        cost_history.append(cost) #Appending cost after each epoch\n",
    "\n",
    "\n",
    "    return initial_params_abcd,params_dict,grads_cache_,cost_history,y_pred,cache_tray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9503438",
   "metadata": {
    "papermill": {
     "duration": 0.044255,
     "end_time": "2021-07-31T18:07:56.886620",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.842365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model by invoking the above UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be88389c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:56.980266Z",
     "iopub.status.busy": "2021-07-31T18:07:56.979256Z",
     "iopub.status.idle": "2021-07-31T18:07:58.790931Z",
     "shell.execute_reply": "2021-07-31T18:07:58.791477Z",
     "shell.execute_reply.started": "2021-07-31T17:54:43.094006Z"
    },
    "papermill": {
     "duration": 1.858036,
     "end_time": "2021-07-31T18:07:58.791634",
     "exception": false,
     "start_time": "2021-07-31T18:07:56.933598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rows : 792\n",
      "Total Batches to create in each epoch/iter : 31\n",
      "Total Batches of X: 31\n",
      "Total Batches of y: 31\n",
      "#----------------- Initial params ------------------#\n",
      "{'w1': array([[0.05228254, 0.02531952, 0.04315081, 0.03784554, 0.02468908,\n",
      "        0.02025509, 0.00896213, 0.04571792, 0.08958744, 0.07571408,\n",
      "        0.09978707],\n",
      "       [0.02293467, 0.06474561, 0.05964706, 0.09538497, 0.00397252,\n",
      "        0.04659595, 0.09387333, 0.03028049, 0.07306254, 0.03333097,\n",
      "        0.02709394],\n",
      "       [0.0255443 , 0.0038734 , 0.07199064, 0.09850637, 0.0270072 ,\n",
      "        0.05540468, 0.02834322, 0.09943038, 0.05951308, 0.07319472,\n",
      "        0.09833763]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'w2': array([[0.06905196, 0.09634047, 0.08159499]]), 'b2': array([[0.]])}\n",
      "#----------------------------------- Epoch : 100 --------------------------------------#\n",
      "cost : 0.5691891719600729\n",
      "#----------------------------------- Epoch : 200 --------------------------------------#\n",
      "cost : 0.4550048617844577\n",
      "#----------------------------------- Epoch : 300 --------------------------------------#\n",
      "cost : 0.40444944671717203\n",
      "#----------------------------------- Epoch : 400 --------------------------------------#\n",
      "cost : 0.38170635136229203\n",
      "#----------------------------------- Epoch : 500 --------------------------------------#\n",
      "cost : 0.3725232190335974\n"
     ]
    }
   ],
   "source": [
    "#Defining hyper-parameters for ANN\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "n_hidden = 2       #No of hidden layers\n",
    "alpha = 0.003      #Learning_rate\n",
    "n_iters = 501      #Total epochs\n",
    "hidden_size_list = [0,3,1]               #first element will be 0 and not counted in hidden layers\n",
    "activation_list = [0,'relu','sigmoid']   #first element will be 0 and not counted in hidden layers\n",
    "batch_size = 25    #Batch wise gradient descent\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "initial_params_train,params_dict_train,grads,cost_history_train,y_pred_train,cache_tray = ANN_train_sgd(data_x_overall=X_arr,\n",
    "                                                                                                       data_y_overall=y_arr,\n",
    "                                                                                                       batch_size=batch_size,\n",
    "                                                                                                       alpha=alpha,\n",
    "                                                                                                       n_iters=n_iters,\n",
    "                                                                                                       n_hidden=n_hidden,\n",
    "                                                                                                       hidden_size_list=hidden_size_list,\n",
    "                                                                                                       activation_list=activation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d671d5",
   "metadata": {
    "papermill": {
     "duration": 0.026064,
     "end_time": "2021-07-31T18:07:58.844803",
     "exception": false,
     "start_time": "2021-07-31T18:07:58.818739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cost-Epoch plot for the manual ANN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f891215f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:58.919263Z",
     "iopub.status.busy": "2021-07-31T18:07:58.918617Z",
     "iopub.status.idle": "2021-07-31T18:07:59.131836Z",
     "shell.execute_reply": "2021-07-31T18:07:59.131185Z",
     "shell.execute_reply.started": "2021-07-31T17:54:50.884102Z"
    },
    "papermill": {
     "duration": 0.258466,
     "end_time": "2021-07-31T18:07:59.131966",
     "exception": false,
     "start_time": "2021-07-31T18:07:58.873500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'epochs'),\n",
       " Text(0, 0.5, 'cost'),\n",
       " Text(0.5, 1.0, 'Cost vs epoch plot for Manual ANN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cost plot over epochs (1 value at end of each epoch) - over the last batch\n",
    "ax = sns.lineplot(x=list(range(n_iters)),y=cost_history_train)\n",
    "ax.set(xlabel='epochs',ylabel='cost',title='Cost vs epoch plot for Manual ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff100ead",
   "metadata": {
    "papermill": {
     "duration": 0.043133,
     "end_time": "2021-07-31T18:07:59.218228",
     "exception": false,
     "start_time": "2021-07-31T18:07:59.175095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8473c21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:59.312521Z",
     "iopub.status.busy": "2021-07-31T18:07:59.311143Z",
     "iopub.status.idle": "2021-07-31T18:07:59.319228Z",
     "shell.execute_reply": "2021-07-31T18:07:59.318599Z",
     "shell.execute_reply.started": "2021-07-31T17:54:56.739041Z"
    },
    "papermill": {
     "duration": 0.057864,
     "end_time": "2021-07-31T18:07:59.319364",
     "exception": false,
     "start_time": "2021-07-31T18:07:59.261500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prediction array : (1, 100)\n",
      "Unique predictions : [0. 1.]\n",
      "Unique of predict proba : [0.1917 0.1998 0.2234 0.228  0.2291 0.2316 0.2583 0.2677 0.2798 0.2986\n",
      " 0.3582 0.3745 0.3956 0.4157 0.4286 0.4593 0.506  0.5069 0.5259 0.536\n",
      " 0.7118 0.7347 0.754  0.8224 0.851  0.8967 0.9346 0.9663 0.968  0.9718\n",
      " 0.9734 0.9735 0.974  0.9749 0.9839 0.9879 0.9892 0.9949 0.9955 0.9958\n",
      " 0.9972 0.9977] \n",
      "\n",
      "#--------------------- Evaluation ----------------------#\n",
      "ROC AUC of test set : 0.7960069444444444\n",
      "Accuracy of test set : 0.84\n"
     ]
    }
   ],
   "source": [
    "cache,preds_proba,manual_preds = prediction(params=params_dict_train,\n",
    "                                            test_x=X_test_arr,\n",
    "                                            n_hidden=n_hidden,\n",
    "                                            hidden_size_list=hidden_size_list,\n",
    "                                            activation_list=activation_list,\n",
    "                                            threshold=0.5)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "print('Shape of prediction array :',preds_proba.shape)\n",
    "print('Unique predictions :',np.unique(manual_preds))\n",
    "print('Unique of predict proba :',np.unique(preds_proba),'\\n')\n",
    "\n",
    "print('#--------------------- Evaluation ----------------------#')\n",
    "#Evaluation of the predictions\n",
    "print('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),manual_preds.ravel()))\n",
    "print('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),manual_preds.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b99494",
   "metadata": {
    "papermill": {
     "duration": 0.042757,
     "end_time": "2021-07-31T18:07:59.404979",
     "exception": false,
     "start_time": "2021-07-31T18:07:59.362222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking with Keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca196c8",
   "metadata": {
    "papermill": {
     "duration": 0.041887,
     "end_time": "2021-07-31T18:07:59.488930",
     "exception": false,
     "start_time": "2021-07-31T18:07:59.447043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8df24ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:07:59.578279Z",
     "iopub.status.busy": "2021-07-31T18:07:59.577549Z",
     "iopub.status.idle": "2021-07-31T18:08:04.886575Z",
     "shell.execute_reply": "2021-07-31T18:08:04.885982Z",
     "shell.execute_reply.started": "2021-07-31T17:52:00.111236Z"
    },
    "papermill": {
     "duration": 5.355619,
     "end_time": "2021-07-31T18:08:04.886742",
     "exception": false,
     "start_time": "2021-07-31T18:07:59.531123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.models\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bf40b",
   "metadata": {
    "papermill": {
     "duration": 0.042855,
     "end_time": "2021-07-31T18:08:04.972287",
     "exception": false,
     "start_time": "2021-07-31T18:08:04.929432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining the model with same specifications as manual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc259d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:08:05.064044Z",
     "iopub.status.busy": "2021-07-31T18:08:05.063336Z",
     "iopub.status.idle": "2021-07-31T18:08:05.327506Z",
     "shell.execute_reply": "2021-07-31T18:08:05.326755Z",
     "shell.execute_reply.started": "2021-07-31T17:52:04.390917Z"
    },
    "papermill": {
     "duration": 0.312952,
     "end_time": "2021-07-31T18:08:05.327694",
     "exception": false,
     "start_time": "2021-07-31T18:08:05.014742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ANN_keras\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 11)]              0         \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 3)                 36        \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def ANN_keras(x):\n",
    "    \n",
    "    input_ = tfl.Input(shape=(x.shape[1],))\n",
    "    \n",
    "    x = tfl.Dense(3,activation='relu', name = 'Dense_3')(input_) #Layer 1\n",
    "    \n",
    "    preds = tfl.Dense(1, activation=\"sigmoid\", name=\"pred\")(x) #Output layer\n",
    "    \n",
    "    model = Model(input_, preds, name=\"ANN_keras\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)) #Stochastic Gradient Descent with specified alpha\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = ANN_keras(X_arr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a479e",
   "metadata": {
    "papermill": {
     "duration": 0.043048,
     "end_time": "2021-07-31T18:08:05.414831",
     "exception": false,
     "start_time": "2021-07-31T18:08:05.371783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90c386cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:08:05.506884Z",
     "iopub.status.busy": "2021-07-31T18:08:05.505867Z",
     "iopub.status.idle": "2021-07-31T18:08:42.601559Z",
     "shell.execute_reply": "2021-07-31T18:08:42.600965Z",
     "shell.execute_reply.started": "2021-07-31T18:01:01.193634Z"
    },
    "papermill": {
     "duration": 37.143217,
     "end_time": "2021-07-31T18:08:42.601748",
     "exception": false,
     "start_time": "2021-07-31T18:08:05.458531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/501\n",
      "32/32 [==============================] - 1s 15ms/step - loss: 0.8200 - val_loss: 0.7174\n",
      "Epoch 2/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7529 - val_loss: 0.7043\n",
      "Epoch 3/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7632 - val_loss: 0.6926\n",
      "Epoch 4/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7283 - val_loss: 0.6822\n",
      "Epoch 5/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6751 - val_loss: 0.6729\n",
      "Epoch 6/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7197 - val_loss: 0.6648\n",
      "Epoch 7/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7178 - val_loss: 0.6576\n",
      "Epoch 8/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6739 - val_loss: 0.6509\n",
      "Epoch 9/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6819 - val_loss: 0.6449\n",
      "Epoch 10/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6866 - val_loss: 0.6395\n",
      "Epoch 11/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6823 - val_loss: 0.6345\n",
      "Epoch 12/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6967 - val_loss: 0.6300\n",
      "Epoch 13/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6527 - val_loss: 0.6258\n",
      "Epoch 14/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6719 - val_loss: 0.6219\n",
      "Epoch 15/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6253 - val_loss: 0.6182\n",
      "Epoch 16/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6483 - val_loss: 0.6148\n",
      "Epoch 17/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.6116\n",
      "Epoch 18/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6501 - val_loss: 0.6085\n",
      "Epoch 19/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6397 - val_loss: 0.6058\n",
      "Epoch 20/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6031\n",
      "Epoch 21/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6360 - val_loss: 0.6007\n",
      "Epoch 22/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6373 - val_loss: 0.5984\n",
      "Epoch 23/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6537 - val_loss: 0.5961\n",
      "Epoch 24/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 0.5941\n",
      "Epoch 25/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6206 - val_loss: 0.5921\n",
      "Epoch 26/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6180 - val_loss: 0.5903\n",
      "Epoch 27/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6279 - val_loss: 0.5886\n",
      "Epoch 28/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6408 - val_loss: 0.5869\n",
      "Epoch 29/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6254 - val_loss: 0.5853\n",
      "Epoch 30/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6246 - val_loss: 0.5837\n",
      "Epoch 31/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6143 - val_loss: 0.5821\n",
      "Epoch 32/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 0.5807\n",
      "Epoch 33/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6275 - val_loss: 0.5793\n",
      "Epoch 34/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6120 - val_loss: 0.5780\n",
      "Epoch 35/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5767\n",
      "Epoch 36/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6151 - val_loss: 0.5754\n",
      "Epoch 37/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.5742\n",
      "Epoch 38/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5982 - val_loss: 0.5730\n",
      "Epoch 39/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.5718\n",
      "Epoch 40/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5707\n",
      "Epoch 41/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5696\n",
      "Epoch 42/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.5685\n",
      "Epoch 43/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6057 - val_loss: 0.5674\n",
      "Epoch 44/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5664\n",
      "Epoch 45/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.5654\n",
      "Epoch 46/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5659 - val_loss: 0.5644\n",
      "Epoch 47/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6125 - val_loss: 0.5635\n",
      "Epoch 48/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6160 - val_loss: 0.5625\n",
      "Epoch 49/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6002 - val_loss: 0.5616\n",
      "Epoch 50/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5974 - val_loss: 0.5607\n",
      "Epoch 51/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5599\n",
      "Epoch 52/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5590\n",
      "Epoch 53/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.5582\n",
      "Epoch 54/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5573\n",
      "Epoch 55/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.5566\n",
      "Epoch 56/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5558\n",
      "Epoch 57/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 0.5550\n",
      "Epoch 58/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5641 - val_loss: 0.5543\n",
      "Epoch 59/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5539 - val_loss: 0.5535\n",
      "Epoch 60/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5816 - val_loss: 0.5528\n",
      "Epoch 61/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5521\n",
      "Epoch 62/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 0.5514\n",
      "Epoch 63/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5652 - val_loss: 0.5508\n",
      "Epoch 64/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 0.5501\n",
      "Epoch 65/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5904 - val_loss: 0.5495\n",
      "Epoch 66/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5489\n",
      "Epoch 67/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5688 - val_loss: 0.5483\n",
      "Epoch 68/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.5477\n",
      "Epoch 69/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5637 - val_loss: 0.5471\n",
      "Epoch 70/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5465\n",
      "Epoch 71/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5814 - val_loss: 0.5459\n",
      "Epoch 72/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5492 - val_loss: 0.5453\n",
      "Epoch 73/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5834 - val_loss: 0.5447\n",
      "Epoch 74/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5442\n",
      "Epoch 75/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5539 - val_loss: 0.5436\n",
      "Epoch 76/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5430\n",
      "Epoch 77/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5481 - val_loss: 0.5425\n",
      "Epoch 78/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 0.5419\n",
      "Epoch 79/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5531 - val_loss: 0.5414\n",
      "Epoch 80/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5609 - val_loss: 0.5408\n",
      "Epoch 81/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5360 - val_loss: 0.5403\n",
      "Epoch 82/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5671 - val_loss: 0.5397\n",
      "Epoch 83/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5632 - val_loss: 0.5392\n",
      "Epoch 84/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5574 - val_loss: 0.5387\n",
      "Epoch 85/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5666 - val_loss: 0.5381\n",
      "Epoch 86/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5634 - val_loss: 0.5376\n",
      "Epoch 87/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5744 - val_loss: 0.5371\n",
      "Epoch 88/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5644 - val_loss: 0.5367\n",
      "Epoch 89/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5498 - val_loss: 0.5362\n",
      "Epoch 90/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5357\n",
      "Epoch 91/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5600 - val_loss: 0.5352\n",
      "Epoch 92/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 0.5347\n",
      "Epoch 93/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5494 - val_loss: 0.5343\n",
      "Epoch 94/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5692 - val_loss: 0.5339\n",
      "Epoch 95/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5579 - val_loss: 0.5335\n",
      "Epoch 96/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5586 - val_loss: 0.5331\n",
      "Epoch 97/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5477 - val_loss: 0.5327\n",
      "Epoch 98/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5536 - val_loss: 0.5323\n",
      "Epoch 99/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5525 - val_loss: 0.5319\n",
      "Epoch 100/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5316\n",
      "Epoch 101/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5603 - val_loss: 0.5312\n",
      "Epoch 102/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 0.5308\n",
      "Epoch 103/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5305\n",
      "Epoch 104/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5527 - val_loss: 0.5301\n",
      "Epoch 105/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5381 - val_loss: 0.5298\n",
      "Epoch 106/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5295\n",
      "Epoch 107/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5292\n",
      "Epoch 108/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5627 - val_loss: 0.5289\n",
      "Epoch 109/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5411 - val_loss: 0.5287\n",
      "Epoch 110/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5498 - val_loss: 0.5284\n",
      "Epoch 111/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5281\n",
      "Epoch 112/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5588 - val_loss: 0.5278\n",
      "Epoch 113/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5276\n",
      "Epoch 114/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5273\n",
      "Epoch 115/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 0.5271\n",
      "Epoch 116/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.5269\n",
      "Epoch 117/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 0.5267\n",
      "Epoch 118/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5266\n",
      "Epoch 119/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5528 - val_loss: 0.5264\n",
      "Epoch 120/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5241 - val_loss: 0.5263\n",
      "Epoch 121/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5482 - val_loss: 0.5261\n",
      "Epoch 122/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5139 - val_loss: 0.5260\n",
      "Epoch 123/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 0.5259\n",
      "Epoch 124/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5257\n",
      "Epoch 125/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5240 - val_loss: 0.5256\n",
      "Epoch 126/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.5254\n",
      "Epoch 127/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5080 - val_loss: 0.5253\n",
      "Epoch 128/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5595 - val_loss: 0.5252\n",
      "Epoch 129/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5567 - val_loss: 0.5251\n",
      "Epoch 130/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5254 - val_loss: 0.5250\n",
      "Epoch 131/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 0.5249\n",
      "Epoch 132/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5453 - val_loss: 0.5247\n",
      "Epoch 133/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5247\n",
      "Epoch 134/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5181 - val_loss: 0.5246\n",
      "Epoch 135/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5196 - val_loss: 0.5245\n",
      "Epoch 136/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5029 - val_loss: 0.5244\n",
      "Epoch 137/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.5243\n",
      "Epoch 138/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5186 - val_loss: 0.5243\n",
      "Epoch 139/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.5243\n",
      "Epoch 140/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5107 - val_loss: 0.5242\n",
      "Epoch 141/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5179 - val_loss: 0.5242\n",
      "Epoch 142/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.5241\n",
      "Epoch 143/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5104 - val_loss: 0.5241\n",
      "Epoch 144/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 0.5241\n",
      "Epoch 145/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 0.5240\n",
      "Epoch 146/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5370 - val_loss: 0.5239\n",
      "Epoch 147/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5280 - val_loss: 0.5238\n",
      "Epoch 148/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5237\n",
      "Epoch 149/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5333 - val_loss: 0.5237\n",
      "Epoch 150/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5027 - val_loss: 0.5236\n",
      "Epoch 151/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5163 - val_loss: 0.5235\n",
      "Epoch 152/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 0.5234\n",
      "Epoch 153/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5085 - val_loss: 0.5234\n",
      "Epoch 154/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5213 - val_loss: 0.5233\n",
      "Epoch 155/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5046 - val_loss: 0.5232\n",
      "Epoch 156/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5117 - val_loss: 0.5231\n",
      "Epoch 157/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5097 - val_loss: 0.5230\n",
      "Epoch 158/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4923 - val_loss: 0.5229\n",
      "Epoch 159/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5228\n",
      "Epoch 160/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5136 - val_loss: 0.5227\n",
      "Epoch 161/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4884 - val_loss: 0.5226\n",
      "Epoch 162/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5105 - val_loss: 0.5225\n",
      "Epoch 163/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5200 - val_loss: 0.5223\n",
      "Epoch 164/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5004 - val_loss: 0.5222\n",
      "Epoch 165/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5054 - val_loss: 0.5220\n",
      "Epoch 166/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4941 - val_loss: 0.5218\n",
      "Epoch 167/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5320 - val_loss: 0.5217\n",
      "Epoch 168/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 0.5215\n",
      "Epoch 169/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5255 - val_loss: 0.5214\n",
      "Epoch 170/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.5212\n",
      "Epoch 171/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 0.5211\n",
      "Epoch 172/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5070 - val_loss: 0.5210\n",
      "Epoch 173/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5149 - val_loss: 0.5208\n",
      "Epoch 174/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 0.5206\n",
      "Epoch 175/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4885 - val_loss: 0.5205\n",
      "Epoch 176/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4932 - val_loss: 0.5203\n",
      "Epoch 177/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5006 - val_loss: 0.5202\n",
      "Epoch 178/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5091 - val_loss: 0.5200\n",
      "Epoch 179/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4822 - val_loss: 0.5198\n",
      "Epoch 180/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5000 - val_loss: 0.5195\n",
      "Epoch 181/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4853 - val_loss: 0.5193\n",
      "Epoch 182/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4886 - val_loss: 0.5191\n",
      "Epoch 183/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.5189\n",
      "Epoch 184/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4801 - val_loss: 0.5187\n",
      "Epoch 185/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4991 - val_loss: 0.5185\n",
      "Epoch 186/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5040 - val_loss: 0.5182\n",
      "Epoch 187/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5084 - val_loss: 0.5180\n",
      "Epoch 188/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4989 - val_loss: 0.5178\n",
      "Epoch 189/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.5176\n",
      "Epoch 190/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4907 - val_loss: 0.5174\n",
      "Epoch 191/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5071 - val_loss: 0.5172\n",
      "Epoch 192/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4831 - val_loss: 0.5170\n",
      "Epoch 193/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5168\n",
      "Epoch 194/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4679 - val_loss: 0.5166\n",
      "Epoch 195/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5049 - val_loss: 0.5164\n",
      "Epoch 196/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5130 - val_loss: 0.5162\n",
      "Epoch 197/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5161\n",
      "Epoch 198/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4969 - val_loss: 0.5158\n",
      "Epoch 199/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4999 - val_loss: 0.5156\n",
      "Epoch 200/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4991 - val_loss: 0.5154\n",
      "Epoch 201/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5139 - val_loss: 0.5152\n",
      "Epoch 202/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4956 - val_loss: 0.5150\n",
      "Epoch 203/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4959 - val_loss: 0.5148\n",
      "Epoch 204/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4742 - val_loss: 0.5146\n",
      "Epoch 205/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4909 - val_loss: 0.5144\n",
      "Epoch 206/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5092 - val_loss: 0.5142\n",
      "Epoch 207/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5107 - val_loss: 0.5140\n",
      "Epoch 208/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 0.5138\n",
      "Epoch 209/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5195 - val_loss: 0.5136\n",
      "Epoch 210/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5029 - val_loss: 0.5134\n",
      "Epoch 211/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - val_loss: 0.5131\n",
      "Epoch 212/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4884 - val_loss: 0.5129\n",
      "Epoch 213/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4879 - val_loss: 0.5127\n",
      "Epoch 214/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4830 - val_loss: 0.5125\n",
      "Epoch 215/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5005 - val_loss: 0.5122\n",
      "Epoch 216/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4981 - val_loss: 0.5120\n",
      "Epoch 217/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4916 - val_loss: 0.5117\n",
      "Epoch 218/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5186 - val_loss: 0.5115\n",
      "Epoch 219/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4979 - val_loss: 0.5112\n",
      "Epoch 220/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4905 - val_loss: 0.5109\n",
      "Epoch 221/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4877 - val_loss: 0.5107\n",
      "Epoch 222/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5078 - val_loss: 0.5104\n",
      "Epoch 223/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4874 - val_loss: 0.5102\n",
      "Epoch 224/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4891 - val_loss: 0.5100\n",
      "Epoch 225/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4993 - val_loss: 0.5097\n",
      "Epoch 226/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4780 - val_loss: 0.5095\n",
      "Epoch 227/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4876 - val_loss: 0.5092\n",
      "Epoch 228/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4978 - val_loss: 0.5090\n",
      "Epoch 229/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4906 - val_loss: 0.5087\n",
      "Epoch 230/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4990 - val_loss: 0.5085\n",
      "Epoch 231/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5134 - val_loss: 0.5082\n",
      "Epoch 232/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5025 - val_loss: 0.5080\n",
      "Epoch 233/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4840 - val_loss: 0.5077\n",
      "Epoch 234/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4873 - val_loss: 0.5075\n",
      "Epoch 235/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4649 - val_loss: 0.5073\n",
      "Epoch 236/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4739 - val_loss: 0.5070\n",
      "Epoch 237/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4984 - val_loss: 0.5068\n",
      "Epoch 238/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4896 - val_loss: 0.5066\n",
      "Epoch 239/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4794 - val_loss: 0.5064\n",
      "Epoch 240/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5064 - val_loss: 0.5061\n",
      "Epoch 241/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4700 - val_loss: 0.5058\n",
      "Epoch 242/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5029 - val_loss: 0.5056\n",
      "Epoch 243/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4845 - val_loss: 0.5054\n",
      "Epoch 244/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4584 - val_loss: 0.5051\n",
      "Epoch 245/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4824 - val_loss: 0.5048\n",
      "Epoch 246/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4804 - val_loss: 0.5046\n",
      "Epoch 247/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4833 - val_loss: 0.5043\n",
      "Epoch 248/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4982 - val_loss: 0.5040\n",
      "Epoch 249/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4916 - val_loss: 0.5037\n",
      "Epoch 250/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.5035\n",
      "Epoch 251/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4680 - val_loss: 0.5032\n",
      "Epoch 252/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4710 - val_loss: 0.5030\n",
      "Epoch 253/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4985 - val_loss: 0.5027\n",
      "Epoch 254/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5015 - val_loss: 0.5024\n",
      "Epoch 255/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4805 - val_loss: 0.5022\n",
      "Epoch 256/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5072 - val_loss: 0.5019\n",
      "Epoch 257/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4807 - val_loss: 0.5016\n",
      "Epoch 258/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5201 - val_loss: 0.5013\n",
      "Epoch 259/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4741 - val_loss: 0.5011\n",
      "Epoch 260/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5115 - val_loss: 0.5008\n",
      "Epoch 261/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4841 - val_loss: 0.5006\n",
      "Epoch 262/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4805 - val_loss: 0.5003\n",
      "Epoch 263/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4735 - val_loss: 0.5001\n",
      "Epoch 264/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4927 - val_loss: 0.4998\n",
      "Epoch 265/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4841 - val_loss: 0.4996\n",
      "Epoch 266/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4679 - val_loss: 0.4993\n",
      "Epoch 267/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4888 - val_loss: 0.4991\n",
      "Epoch 268/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4837 - val_loss: 0.4988\n",
      "Epoch 269/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4771 - val_loss: 0.4986\n",
      "Epoch 270/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4926 - val_loss: 0.4983\n",
      "Epoch 271/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.4980\n",
      "Epoch 272/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4937 - val_loss: 0.4978\n",
      "Epoch 273/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5427 - val_loss: 0.4975\n",
      "Epoch 274/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5036 - val_loss: 0.4972\n",
      "Epoch 275/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4738 - val_loss: 0.4970\n",
      "Epoch 276/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4954 - val_loss: 0.4967\n",
      "Epoch 277/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4738 - val_loss: 0.4965\n",
      "Epoch 278/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.4962\n",
      "Epoch 279/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4692 - val_loss: 0.4960\n",
      "Epoch 280/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4676 - val_loss: 0.4958\n",
      "Epoch 281/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4784 - val_loss: 0.4955\n",
      "Epoch 282/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4865 - val_loss: 0.4953\n",
      "Epoch 283/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4573 - val_loss: 0.4951\n",
      "Epoch 284/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5086 - val_loss: 0.4948\n",
      "Epoch 285/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4764 - val_loss: 0.4946\n",
      "Epoch 286/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.4943\n",
      "Epoch 287/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5031 - val_loss: 0.4941\n",
      "Epoch 288/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4839 - val_loss: 0.4939\n",
      "Epoch 289/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4850 - val_loss: 0.4937\n",
      "Epoch 290/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5021 - val_loss: 0.4934\n",
      "Epoch 291/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.4932\n",
      "Epoch 292/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4873 - val_loss: 0.4930\n",
      "Epoch 293/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5049 - val_loss: 0.4927\n",
      "Epoch 294/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4895 - val_loss: 0.4925\n",
      "Epoch 295/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4534 - val_loss: 0.4922\n",
      "Epoch 296/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4709 - val_loss: 0.4920\n",
      "Epoch 297/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4880 - val_loss: 0.4918\n",
      "Epoch 298/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4606 - val_loss: 0.4915\n",
      "Epoch 299/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.4913\n",
      "Epoch 300/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4889 - val_loss: 0.4911\n",
      "Epoch 301/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4943 - val_loss: 0.4910\n",
      "Epoch 302/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 0.4907\n",
      "Epoch 303/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4947 - val_loss: 0.4905\n",
      "Epoch 304/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4946 - val_loss: 0.4903\n",
      "Epoch 305/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4955 - val_loss: 0.4901\n",
      "Epoch 306/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4733 - val_loss: 0.4899\n",
      "Epoch 307/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 0.4897\n",
      "Epoch 308/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4762 - val_loss: 0.4895\n",
      "Epoch 309/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4935 - val_loss: 0.4893\n",
      "Epoch 310/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4726 - val_loss: 0.4890\n",
      "Epoch 311/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4835 - val_loss: 0.4888\n",
      "Epoch 312/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4759 - val_loss: 0.4886\n",
      "Epoch 313/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4506 - val_loss: 0.4884\n",
      "Epoch 314/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4755 - val_loss: 0.4882\n",
      "Epoch 315/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5006 - val_loss: 0.4880\n",
      "Epoch 316/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5029 - val_loss: 0.4877\n",
      "Epoch 317/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5140 - val_loss: 0.4875\n",
      "Epoch 318/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4819 - val_loss: 0.4873\n",
      "Epoch 319/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4870\n",
      "Epoch 320/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.4868\n",
      "Epoch 321/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4761 - val_loss: 0.4866\n",
      "Epoch 322/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4783 - val_loss: 0.4863\n",
      "Epoch 323/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4859 - val_loss: 0.4861\n",
      "Epoch 324/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4829 - val_loss: 0.4858\n",
      "Epoch 325/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4898 - val_loss: 0.4856\n",
      "Epoch 326/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4590 - val_loss: 0.4854\n",
      "Epoch 327/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4760 - val_loss: 0.4851\n",
      "Epoch 328/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4919 - val_loss: 0.4849\n",
      "Epoch 329/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4724 - val_loss: 0.4847\n",
      "Epoch 330/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4828 - val_loss: 0.4845\n",
      "Epoch 331/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4638 - val_loss: 0.4843\n",
      "Epoch 332/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4472 - val_loss: 0.4842\n",
      "Epoch 333/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4737 - val_loss: 0.4840\n",
      "Epoch 334/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4611 - val_loss: 0.4838\n",
      "Epoch 335/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4753 - val_loss: 0.4836\n",
      "Epoch 336/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4573 - val_loss: 0.4834\n",
      "Epoch 337/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4495 - val_loss: 0.4832\n",
      "Epoch 338/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4830\n",
      "Epoch 339/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4490 - val_loss: 0.4828\n",
      "Epoch 340/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4688 - val_loss: 0.4826\n",
      "Epoch 341/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4941 - val_loss: 0.4825\n",
      "Epoch 342/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4591 - val_loss: 0.4824\n",
      "Epoch 343/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4802 - val_loss: 0.4822\n",
      "Epoch 344/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4978 - val_loss: 0.4820\n",
      "Epoch 345/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4964 - val_loss: 0.4819\n",
      "Epoch 346/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5082 - val_loss: 0.4816\n",
      "Epoch 347/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4563 - val_loss: 0.4815\n",
      "Epoch 348/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4962 - val_loss: 0.4813\n",
      "Epoch 349/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4564 - val_loss: 0.4811\n",
      "Epoch 350/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4766 - val_loss: 0.4809\n",
      "Epoch 351/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - val_loss: 0.4808\n",
      "Epoch 352/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4628 - val_loss: 0.4806\n",
      "Epoch 353/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4623 - val_loss: 0.4804\n",
      "Epoch 354/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4575 - val_loss: 0.4803\n",
      "Epoch 355/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4632 - val_loss: 0.4801\n",
      "Epoch 356/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4733 - val_loss: 0.4799\n",
      "Epoch 357/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4758 - val_loss: 0.4797\n",
      "Epoch 358/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.4796\n",
      "Epoch 359/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4324 - val_loss: 0.4794\n",
      "Epoch 360/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4775 - val_loss: 0.4792\n",
      "Epoch 361/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4791\n",
      "Epoch 362/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4791 - val_loss: 0.4789\n",
      "Epoch 363/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4699 - val_loss: 0.4787\n",
      "Epoch 364/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4598 - val_loss: 0.4786\n",
      "Epoch 365/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4324 - val_loss: 0.4784\n",
      "Epoch 366/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4609 - val_loss: 0.4783\n",
      "Epoch 367/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4602 - val_loss: 0.4781\n",
      "Epoch 368/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5045 - val_loss: 0.4779\n",
      "Epoch 369/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4573 - val_loss: 0.4778\n",
      "Epoch 370/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4389 - val_loss: 0.4776\n",
      "Epoch 371/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4890 - val_loss: 0.4774\n",
      "Epoch 372/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4859 - val_loss: 0.4773\n",
      "Epoch 373/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4893 - val_loss: 0.4771\n",
      "Epoch 374/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4718 - val_loss: 0.4769\n",
      "Epoch 375/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4667 - val_loss: 0.4767\n",
      "Epoch 376/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4637 - val_loss: 0.4766\n",
      "Epoch 377/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4687 - val_loss: 0.4764\n",
      "Epoch 378/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4667 - val_loss: 0.4762\n",
      "Epoch 379/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4694 - val_loss: 0.4760\n",
      "Epoch 380/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4690 - val_loss: 0.4759\n",
      "Epoch 381/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4659 - val_loss: 0.4757\n",
      "Epoch 382/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.4755\n",
      "Epoch 383/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4666 - val_loss: 0.4754\n",
      "Epoch 384/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 0.4752\n",
      "Epoch 385/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4388 - val_loss: 0.4751\n",
      "Epoch 386/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4616 - val_loss: 0.4750\n",
      "Epoch 387/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4478 - val_loss: 0.4748\n",
      "Epoch 388/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4848 - val_loss: 0.4747\n",
      "Epoch 389/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4714 - val_loss: 0.4745\n",
      "Epoch 390/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4782 - val_loss: 0.4743\n",
      "Epoch 391/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4600 - val_loss: 0.4742\n",
      "Epoch 392/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4520 - val_loss: 0.4740\n",
      "Epoch 393/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4961 - val_loss: 0.4738\n",
      "Epoch 394/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4656 - val_loss: 0.4737\n",
      "Epoch 395/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4482 - val_loss: 0.4736\n",
      "Epoch 396/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4798 - val_loss: 0.4734\n",
      "Epoch 397/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4525 - val_loss: 0.4733\n",
      "Epoch 398/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4721 - val_loss: 0.4732\n",
      "Epoch 399/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4741 - val_loss: 0.4731\n",
      "Epoch 400/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4473 - val_loss: 0.4729\n",
      "Epoch 401/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.4728\n",
      "Epoch 402/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4417 - val_loss: 0.4727\n",
      "Epoch 403/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4954 - val_loss: 0.4725\n",
      "Epoch 404/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4569 - val_loss: 0.4724\n",
      "Epoch 405/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4661 - val_loss: 0.4722\n",
      "Epoch 406/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4703 - val_loss: 0.4721\n",
      "Epoch 407/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4836 - val_loss: 0.4720\n",
      "Epoch 408/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4553 - val_loss: 0.4718\n",
      "Epoch 409/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4967 - val_loss: 0.4717\n",
      "Epoch 410/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4752 - val_loss: 0.4715\n",
      "Epoch 411/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4470 - val_loss: 0.4714\n",
      "Epoch 412/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4692 - val_loss: 0.4712\n",
      "Epoch 413/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4253 - val_loss: 0.4711\n",
      "Epoch 414/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4746 - val_loss: 0.4710\n",
      "Epoch 415/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4770 - val_loss: 0.4708\n",
      "Epoch 416/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4708\n",
      "Epoch 417/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4598 - val_loss: 0.4706\n",
      "Epoch 418/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4851 - val_loss: 0.4705\n",
      "Epoch 419/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4737 - val_loss: 0.4703\n",
      "Epoch 420/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4487 - val_loss: 0.4702\n",
      "Epoch 421/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4548 - val_loss: 0.4701\n",
      "Epoch 422/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4690 - val_loss: 0.4700\n",
      "Epoch 423/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4822 - val_loss: 0.4698\n",
      "Epoch 424/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4427 - val_loss: 0.4696\n",
      "Epoch 425/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4892 - val_loss: 0.4695\n",
      "Epoch 426/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4598 - val_loss: 0.4694\n",
      "Epoch 427/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.4692\n",
      "Epoch 428/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4467 - val_loss: 0.4691\n",
      "Epoch 429/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4330 - val_loss: 0.4689\n",
      "Epoch 430/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4869 - val_loss: 0.4688\n",
      "Epoch 431/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4858 - val_loss: 0.4686\n",
      "Epoch 432/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4819 - val_loss: 0.4685\n",
      "Epoch 433/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4598 - val_loss: 0.4683\n",
      "Epoch 434/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4882 - val_loss: 0.4682\n",
      "Epoch 435/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4680\n",
      "Epoch 436/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.4679\n",
      "Epoch 437/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4486 - val_loss: 0.4677\n",
      "Epoch 438/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4282 - val_loss: 0.4676\n",
      "Epoch 439/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4685 - val_loss: 0.4674\n",
      "Epoch 440/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4832 - val_loss: 0.4673\n",
      "Epoch 441/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4798 - val_loss: 0.4671\n",
      "Epoch 442/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4645 - val_loss: 0.4669\n",
      "Epoch 443/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4681 - val_loss: 0.4668\n",
      "Epoch 444/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4695 - val_loss: 0.4666\n",
      "Epoch 445/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4651 - val_loss: 0.4664\n",
      "Epoch 446/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4332 - val_loss: 0.4663\n",
      "Epoch 447/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4892 - val_loss: 0.4661\n",
      "Epoch 448/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4622 - val_loss: 0.4659\n",
      "Epoch 449/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4529 - val_loss: 0.4658\n",
      "Epoch 450/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4651 - val_loss: 0.4656\n",
      "Epoch 451/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4654\n",
      "Epoch 452/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4446 - val_loss: 0.4653\n",
      "Epoch 453/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.4651\n",
      "Epoch 454/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4785 - val_loss: 0.4649\n",
      "Epoch 455/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4434 - val_loss: 0.4648\n",
      "Epoch 456/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4566 - val_loss: 0.4646\n",
      "Epoch 457/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4294 - val_loss: 0.4644\n",
      "Epoch 458/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4424 - val_loss: 0.4643\n",
      "Epoch 459/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4878 - val_loss: 0.4641\n",
      "Epoch 460/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4403 - val_loss: 0.4640\n",
      "Epoch 461/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4663 - val_loss: 0.4639\n",
      "Epoch 462/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5023 - val_loss: 0.4637\n",
      "Epoch 463/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4843 - val_loss: 0.4636\n",
      "Epoch 464/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.4634\n",
      "Epoch 465/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4445 - val_loss: 0.4633\n",
      "Epoch 466/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4604 - val_loss: 0.4632\n",
      "Epoch 467/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4680 - val_loss: 0.4630\n",
      "Epoch 468/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4965 - val_loss: 0.4629\n",
      "Epoch 469/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4731 - val_loss: 0.4628\n",
      "Epoch 470/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4524 - val_loss: 0.4627\n",
      "Epoch 471/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4459 - val_loss: 0.4625\n",
      "Epoch 472/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4907 - val_loss: 0.4624\n",
      "Epoch 473/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4715 - val_loss: 0.4622\n",
      "Epoch 474/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4423 - val_loss: 0.4621\n",
      "Epoch 475/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4389 - val_loss: 0.4620\n",
      "Epoch 476/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4243 - val_loss: 0.4619\n",
      "Epoch 477/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4771 - val_loss: 0.4618\n",
      "Epoch 478/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4539 - val_loss: 0.4616\n",
      "Epoch 479/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4725 - val_loss: 0.4615\n",
      "Epoch 480/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4617 - val_loss: 0.4613\n",
      "Epoch 481/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4536 - val_loss: 0.4612\n",
      "Epoch 482/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4561 - val_loss: 0.4611\n",
      "Epoch 483/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.4610\n",
      "Epoch 484/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4549 - val_loss: 0.4608\n",
      "Epoch 485/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4760 - val_loss: 0.4607\n",
      "Epoch 486/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4689 - val_loss: 0.4605\n",
      "Epoch 487/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4704 - val_loss: 0.4604\n",
      "Epoch 488/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4826 - val_loss: 0.4603\n",
      "Epoch 489/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4769 - val_loss: 0.4601\n",
      "Epoch 490/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.4600\n",
      "Epoch 491/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4756 - val_loss: 0.4599\n",
      "Epoch 492/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4597\n",
      "Epoch 493/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4434 - val_loss: 0.4596\n",
      "Epoch 494/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4796 - val_loss: 0.4595\n",
      "Epoch 495/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4459 - val_loss: 0.4594\n",
      "Epoch 496/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4672 - val_loss: 0.4592\n",
      "Epoch 497/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4473 - val_loss: 0.4591\n",
      "Epoch 498/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4730 - val_loss: 0.4590\n",
      "Epoch 499/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4442 - val_loss: 0.4589\n",
      "Epoch 500/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5064 - val_loss: 0.4587\n",
      "Epoch 501/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4639 - val_loss: 0.4584\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_arr,y_arr, epochs=n_iters, batch_size=batch_size,\n",
    "                    validation_data = (X_test_arr,y_test_arr),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5469da",
   "metadata": {
    "papermill": {
     "duration": 0.442688,
     "end_time": "2021-07-31T18:08:43.492092",
     "exception": false,
     "start_time": "2021-07-31T18:08:43.049404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predicting through keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41072b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T18:08:44.390015Z",
     "iopub.status.busy": "2021-07-31T18:08:44.388311Z",
     "iopub.status.idle": "2021-07-31T18:08:44.511594Z",
     "shell.execute_reply": "2021-07-31T18:08:44.511047Z",
     "shell.execute_reply.started": "2021-07-31T18:02:19.949694Z"
    },
    "papermill": {
     "duration": 0.575493,
     "end_time": "2021-07-31T18:08:44.511754",
     "exception": false,
     "start_time": "2021-07-31T18:08:43.936261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------- Evaluation ----------------------#\n",
      "ROC AUC of test set : 0.7769097222222222\n",
      "Accuracy of test set : 0.8\n"
     ]
    }
   ],
   "source": [
    "keras_pred = model.predict(X_test_arr)\n",
    "keras_pred = np.where(keras_pred>0.5,1,0)\n",
    "\n",
    "#print(np.unique(keras_pred))\n",
    "print('#--------------------- Evaluation ----------------------#')\n",
    "#Evaluation of the predictions\n",
    "print('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),keras_pred.ravel()))\n",
    "print('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),keras_pred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c78bb",
   "metadata": {
    "papermill": {
     "duration": 0.296567,
     "end_time": "2021-07-31T18:08:45.099051",
     "exception": false,
     "start_time": "2021-07-31T18:08:44.802484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Insights : The manual implementation of ANN is giving very similar predictions as that to the Keras counterparts, indicating the implementation is correct and comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d7ad1",
   "metadata": {
    "papermill": {
     "duration": 0.28347,
     "end_time": "2021-07-31T18:08:45.667010",
     "exception": false,
     "start_time": "2021-07-31T18:08:45.383540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16791d4c",
   "metadata": {
    "papermill": {
     "duration": 0.278251,
     "end_time": "2021-07-31T18:08:46.229419",
     "exception": false,
     "start_time": "2021-07-31T18:08:45.951168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 61.835748,
   "end_time": "2021-07-31T18:08:48.924350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-31T18:07:47.088602",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
