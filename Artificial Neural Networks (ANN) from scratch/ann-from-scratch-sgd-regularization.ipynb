{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8849d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:09.960394Z",
     "iopub.status.busy": "2021-08-01T08:06:09.959762Z",
     "iopub.status.idle": "2021-08-01T08:06:11.072983Z",
     "shell.execute_reply": "2021-08-01T08:06:11.073468Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.213940Z"
    },
    "papermill": {
     "duration": 1.147324,
     "end_time": "2021-08-01T08:06:11.073771",
     "exception": false,
     "start_time": "2021-08-01T08:06:09.926447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train_data.csv\n",
      "/kaggle/input/titanic/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5d894",
   "metadata": {
    "papermill": {
     "duration": 0.022225,
     "end_time": "2021-08-01T08:06:11.119349",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.097124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106fef62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.168011Z",
     "iopub.status.busy": "2021-08-01T08:06:11.167388Z",
     "iopub.status.idle": "2021-08-01T08:06:11.217763Z",
     "shell.execute_reply": "2021-08-01T08:06:11.217221Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.226824Z"
    },
    "papermill": {
     "duration": 0.076992,
     "end_time": "2021-08-01T08:06:11.217902",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.140910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(792, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Sex     Age      Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0            1         0    1  0.2750  0.014151         0         0         1   \n",
       "1            2         1    0  0.4750  0.139136         1         0         0   \n",
       "2            3         1    0  0.3250  0.015469         0         0         1   \n",
       "3            4         1    0  0.4375  0.103644         1         0         0   \n",
       "4            5         0    1  0.4375  0.015713         0         0         1   \n",
       "\n",
       "   Family_size  Emb_1  Emb_2  Emb_3  \n",
       "0          0.1      0      0      1  \n",
       "1          0.1      1      0      0  \n",
       "2          0.0      0      0      1  \n",
       "3          0.1      0      0      1  \n",
       "4          0.0      0      0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ads = pd.read_csv('../input/titanic/train_data.csv')\n",
    "input_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n",
    "#-----------------------------------------------------------------\n",
    "print(input_ads.shape)\n",
    "input_ads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678bedb8",
   "metadata": {
    "papermill": {
     "duration": 0.022109,
     "end_time": "2021-08-01T08:06:11.262132",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.240023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4381c1de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.318847Z",
     "iopub.status.busy": "2021-08-01T08:06:11.315723Z",
     "iopub.status.idle": "2021-08-01T08:06:11.323121Z",
     "shell.execute_reply": "2021-08-01T08:06:11.322673Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.260324Z"
    },
    "papermill": {
     "duration": 0.038797,
     "end_time": "2021-08-01T08:06:11.323279",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.284482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Sex  Age  Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
       "0            0         0    0    0     0         0         0         0   \n",
       "\n",
       "   Family_size  Emb_1  Emb_2  Emb_3  \n",
       "0            0      0      0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(input_ads.isnull().sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1608b",
   "metadata": {
    "papermill": {
     "duration": 0.02237,
     "end_time": "2021-08-01T08:06:11.369684",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.347314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc954410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.423019Z",
     "iopub.status.busy": "2021-08-01T08:06:11.422422Z",
     "iopub.status.idle": "2021-08-01T08:06:11.463711Z",
     "shell.execute_reply": "2021-08-01T08:06:11.462853Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.278437Z"
    },
    "papermill": {
     "duration": 0.071515,
     "end_time": "2021-08-01T08:06:11.463917",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.392402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "      <td>792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>396.500000</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.368244</td>\n",
       "      <td>0.064677</td>\n",
       "      <td>0.243687</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>0.088636</td>\n",
       "      <td>0.185606</td>\n",
       "      <td>0.092172</td>\n",
       "      <td>0.720960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>228.774999</td>\n",
       "      <td>0.487223</td>\n",
       "      <td>0.477980</td>\n",
       "      <td>0.162994</td>\n",
       "      <td>0.100987</td>\n",
       "      <td>0.429577</td>\n",
       "      <td>0.406373</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>0.154485</td>\n",
       "      <td>0.389034</td>\n",
       "      <td>0.289451</td>\n",
       "      <td>0.448811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>198.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>396.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>594.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.061045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>792.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived         Sex         Age        Fare  \\\n",
       "count   792.000000  792.000000  792.000000  792.000000  792.000000   \n",
       "mean    396.500000    0.386364    0.647727    0.368244    0.064677   \n",
       "std     228.774999    0.487223    0.477980    0.162994    0.100987   \n",
       "min       1.000000    0.000000    0.000000    0.008375    0.000000   \n",
       "25%     198.750000    0.000000    0.000000    0.275000    0.015469   \n",
       "50%     396.500000    0.000000    1.000000    0.350000    0.028302   \n",
       "75%     594.250000    1.000000    1.000000    0.437500    0.061045   \n",
       "max     792.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         Pclass_1    Pclass_2    Pclass_3  Family_size       Emb_1  \\\n",
       "count  792.000000  792.000000  792.000000   792.000000  792.000000   \n",
       "mean     0.243687    0.208333    0.547980     0.088636    0.185606   \n",
       "std      0.429577    0.406373    0.498007     0.154485    0.389034   \n",
       "min      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "50%      0.000000    0.000000    1.000000     0.000000    0.000000   \n",
       "75%      0.000000    0.000000    1.000000     0.100000    0.000000   \n",
       "max      1.000000    1.000000    1.000000     1.000000    1.000000   \n",
       "\n",
       "            Emb_2       Emb_3  \n",
       "count  792.000000  792.000000  \n",
       "mean     0.092172    0.720960  \n",
       "std      0.289451    0.448811  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    0.000000  \n",
       "50%      0.000000    1.000000  \n",
       "75%      0.000000    1.000000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ads.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9ca3f",
   "metadata": {
    "papermill": {
     "duration": 0.035164,
     "end_time": "2021-08-01T08:06:11.528033",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.492869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11a4663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.584447Z",
     "iopub.status.busy": "2021-08-01T08:06:11.583819Z",
     "iopub.status.idle": "2021-08-01T08:06:11.588489Z",
     "shell.execute_reply": "2021-08-01T08:06:11.588935Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.330408Z"
    },
    "papermill": {
     "duration": 0.034407,
     "end_time": "2021-08-01T08:06:11.589100",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.554693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    486\n",
       "1    306\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total survived vs not-survived split in the training data\n",
    "input_ads['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649d573",
   "metadata": {
    "papermill": {
     "duration": 0.023158,
     "end_time": "2021-08-01T08:06:11.635278",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.612120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Manipulation of data into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca11a69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.684737Z",
     "iopub.status.busy": "2021-08-01T08:06:11.684119Z",
     "iopub.status.idle": "2021-08-01T08:06:11.709211Z",
     "shell.execute_reply": "2021-08-01T08:06:11.709936Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.338456Z"
    },
    "papermill": {
     "duration": 0.051671,
     "end_time": "2021-08-01T08:06:11.710159",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.658488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train % of total data: 88.78923766816143\n",
      "(792, 11)\n",
      "(100, 11)\n",
      "(792, 1)\n"
     ]
    }
   ],
   "source": [
    "target = 'Survived' #To predict\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Splitting into X & Y datasets (supervised training)\n",
    "X = input_ads[[cols for cols in list(input_ads.columns) if target not in cols]]\n",
    "y = input_ads[target]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Since test data is already placed in the input folder separately, we will just import it\n",
    "test_ads = pd.read_csv('../input/titanic/test_data.csv')\n",
    "test_ads.drop(columns=['Unnamed: 0','Title_1','Title_2','Title_3','Title_4'],inplace=True) #Dropping un-necessary columns\n",
    "\n",
    "#Splitting into X & Y datasets (supervised training)\n",
    "X_test = test_ads[[cols for cols in list(test_ads.columns) if target not in cols]]\n",
    "y_test = test_ads[target]\n",
    "\n",
    "print('Train % of total data:',100 * X.shape[0]/(X.shape[0] + X_test.shape[0]))\n",
    "#--------------------------------------------------------------------------------\n",
    "#Manipulation of datasets for convenience and consistency\n",
    "X_arr = np.array(X)\n",
    "X_test_arr = np.array(X_test)\n",
    "\n",
    "y_arr = np.array(y).reshape(X_arr.shape[0],1)\n",
    "y_test_arr = np.array(y_test).reshape(X_test_arr.shape[0],1)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "#Basic Summary\n",
    "print(X_arr.shape)\n",
    "print(X_test_arr.shape)\n",
    "print(y_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d448eb",
   "metadata": {
    "papermill": {
     "duration": 0.02324,
     "end_time": "2021-08-01T08:06:11.758173",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.734933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Standard scaling the x-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ab1fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.808173Z",
     "iopub.status.busy": "2021-08-01T08:06:11.807515Z",
     "iopub.status.idle": "2021-08-01T08:06:11.814757Z",
     "shell.execute_reply": "2021-08-01T08:06:11.815184Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.360382Z"
    },
    "papermill": {
     "duration": 0.033586,
     "end_time": "2021-08-01T08:06:11.815399",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.781813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.72986525,  0.73746841, -0.57243072, -0.50063632, -0.56762987,\n",
       "        -0.51298918,  0.90823168,  0.07360462, -0.47739604, -0.31863757,\n",
       "         0.62212561],\n",
       "       [-1.72549138, -1.35599029,  0.65538585,  0.73777138,  1.76171137,\n",
       "        -0.51298918, -1.10104065,  0.07360462,  2.0946969 , -0.31863757,\n",
       "        -1.60739242],\n",
       "       [-1.72111752, -1.35599029, -0.26547658, -0.48758178, -0.56762987,\n",
       "        -0.51298918,  0.90823168, -0.57411602, -0.47739604, -0.31863757,\n",
       "         0.62212561]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_arr = scaler.fit_transform(X_arr)\n",
    "X_test_arr = scaler.transform(X_test_arr)\n",
    "\n",
    "#----------------------------------------------------------\n",
    "X_arr[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e228d",
   "metadata": {
    "papermill": {
     "duration": 0.023467,
     "end_time": "2021-08-01T08:06:11.862607",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.839140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Artificial Neural Network (ANN) from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a152ce",
   "metadata": {
    "papermill": {
     "duration": 0.027984,
     "end_time": "2021-08-01T08:06:11.914480",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.886496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDFs for activation, initialization, layer_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022fd15f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:11.980440Z",
     "iopub.status.busy": "2021-08-01T08:06:11.979735Z",
     "iopub.status.idle": "2021-08-01T08:06:11.982236Z",
     "shell.execute_reply": "2021-08-01T08:06:11.982735Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.371795Z"
    },
    "papermill": {
     "duration": 0.041864,
     "end_time": "2021-08-01T08:06:11.982909",
     "exception": false,
     "start_time": "2021-08-01T08:06:11.941045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All popular activation functions\n",
    "def activation_fn(z,type_):\n",
    "    \n",
    "    #print('Activation : ',type_)\n",
    "    \n",
    "    if type_=='linear':\n",
    "        activated_arr = z\n",
    "    \n",
    "    elif type_=='sigmoid':\n",
    "        activated_arr = 1/(1+np.exp(-z))\n",
    "    \n",
    "    elif type_=='relu': \n",
    "        activated_arr = np.maximum(np.zeros(z.shape),z)\n",
    "    \n",
    "    elif type_=='tanh':\n",
    "        activated_arr = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    \n",
    "    elif type_=='leaky_relu':\n",
    "        activated_arr = np.maximum(0.01*z,z)\n",
    "    \n",
    "    elif type_=='softmax':\n",
    "        exp_ = np.exp(z)\n",
    "        exp_sum = np.sum(exp_)\n",
    "        activated_arr = exp_/exp_sum\n",
    "        \n",
    "    return activated_arr\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Initialization of params\n",
    "def generate_param_grid(a_prev,n_hidden,hidden_size_list):\n",
    "    \n",
    "    parameters = {}\n",
    "    features = a_prev.shape[0] #Total features\n",
    "    n_examples = a_prev.shape[1]\n",
    "    \n",
    "    for n_hidden_idx in range(1,n_hidden+1):\n",
    "        \n",
    "        n_hidden_nodes = hidden_size_list[n_hidden_idx] #Should start from 0\n",
    "        \n",
    "        #print('#------------ Layer :',n_hidden_idx,'---- Size :',n_hidden_nodes,'---- Prev features :',features,'------#')\n",
    "\n",
    "        parameters['w' + str(n_hidden_idx)] = np.random.rand(n_hidden_nodes,features) * 0.1 #Xavier Initialization\n",
    "        parameters['b' + str(n_hidden_idx)] = np.zeros((n_hidden_nodes,1)) * 0.1\n",
    "        \n",
    "        features = n_hidden_nodes\n",
    "    \n",
    "    return parameters#Return randomly initiated params\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "#Propagation between z and activation\n",
    "def layer_propagation(a_prev,w,b,activation):\n",
    "    \n",
    "    #print(a_prev.shape)\n",
    "    #print(w.shape)\n",
    "    #print(b.shape)\n",
    "    \n",
    "    z_ = np.dot(w,a_prev) + b\n",
    "    \n",
    "    a = activation_fn(z=z_,\n",
    "                      type_=activation)\n",
    "    \n",
    "    return z_,a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae419dc4",
   "metadata": {
    "papermill": {
     "duration": 0.026659,
     "end_time": "2021-08-01T08:06:12.035053",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.008394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6de506c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.089608Z",
     "iopub.status.busy": "2021-08-01T08:06:12.088962Z",
     "iopub.status.idle": "2021-08-01T08:06:12.090956Z",
     "shell.execute_reply": "2021-08-01T08:06:12.091446Z",
     "shell.execute_reply.started": "2021-08-01T06:51:23.386256Z"
    },
    "papermill": {
     "duration": 0.032632,
     "end_time": "2021-08-01T08:06:12.091614",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.058982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_propagation(params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list):\n",
    "    \n",
    "    cache = {'a0' : data_x.T}\n",
    "    a = data_x.T.copy()\n",
    "    \n",
    "    for layer_idx in range(1,n_hidden+1):\n",
    "        \n",
    "        #print('#---------- Layer :',layer_idx,'-- No of Nodes :',hidden_size_list[layer_idx])\n",
    "        #nodes = hidden_size_list[layer_idx]\n",
    "        activation_ = activation_list[layer_idx]\n",
    "        w_ = params_dict['w'+str(layer_idx)]\n",
    "        b_ = params_dict['b'+str(layer_idx)]\n",
    "        \n",
    "        z,a = layer_propagation(a_prev=a,\n",
    "                                 w=w_,\n",
    "                                 b=b_,\n",
    "                                 activation=activation_)\n",
    "        \n",
    "        cache['z'+str(layer_idx)] = z\n",
    "        cache['a'+str(layer_idx)] = a\n",
    "    \n",
    "    return cache,a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee54ae",
   "metadata": {
    "papermill": {
     "duration": 0.024119,
     "end_time": "2021-08-01T08:06:12.140143",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.116024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for cost calculation, gradient calculation & back-propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7305e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.192432Z",
     "iopub.status.busy": "2021-08-01T08:06:12.191777Z",
     "iopub.status.idle": "2021-08-01T08:06:12.208902Z",
     "shell.execute_reply": "2021-08-01T08:06:12.209354Z",
     "shell.execute_reply.started": "2021-08-01T08:01:33.104899Z"
    },
    "papermill": {
     "duration": 0.044944,
     "end_time": "2021-08-01T08:06:12.209535",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.164591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculation of the total cost incurred by the model\n",
    "def cost_calculation(activation_list,y_true,y_pred,regularization,reg_lambda,params_dict,n_hidden):\n",
    "\n",
    "    #----------------------------------------------------------------\n",
    "    w_ = params_dict['w'+str(n_hidden)]\n",
    "    \n",
    "    #Applying regularizations\n",
    "    if regularization=='L1':\n",
    "        reg = np.sum(abs(w_))\n",
    "    elif regularization=='L2':\n",
    "        reg = 0.5 * np.sum(np.square(w_))\n",
    "    elif regularization==None:\n",
    "        reg = 0\n",
    "    \n",
    "    #----------------------------------------------------------------\n",
    "    if activation_list[-1]=='sigmoid':\n",
    "        #print('sig')\n",
    "        m = y_true.shape[1]\n",
    "        cost = (-1/m) * (np.sum((y_true * np.log(y_pred)) + ((1-y_true) * np.log(1 - y_pred))) + (reg_lambda*reg))\n",
    "        \n",
    "    elif activation_list[-1]=='linear':\n",
    "        \n",
    "        m = y_true.shape[1]\n",
    "        cost = (1/m) * np.sum(np.square(y_true-y_pred)) + (reg_lambda*reg)\n",
    "        \n",
    "     ##-------------------->> Softmax to be added <<----------------------\n",
    "    \n",
    "    return cost\n",
    "\n",
    "#Gradient of the activation functions wrt corresponding z\n",
    "#--------------------------------------------------------------------------------------------\n",
    "#Gradient for each activation type\n",
    "def grad_fn_dz(activation,a):\n",
    "    \n",
    "    if activation=='linear':\n",
    "        grad = 1\n",
    "     \n",
    "    elif activation=='sigmoid':\n",
    "        grad = a*(1-a)\n",
    "        \n",
    "    elif activation=='tanh':\n",
    "        grad = np.square(1-a)\n",
    "        \n",
    "    elif activation=='relu':\n",
    "        grad = np.where(a>=0,1,0)\n",
    "    \n",
    "    elif activation=='leaky_relu':\n",
    "        grad = np.where(a>=0,1,0.01)\n",
    "    \n",
    "    ##-------------------->> Softmax to be added <<----------------------\n",
    "    \n",
    "    return grad\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------\n",
    "#UDF for gradient of loss function wrt last layer\n",
    "def dL_last_layer(activation_list,y_true,y_pred,regularization,reg_lambda,params_dict,n_hidden):\n",
    "    \n",
    "#     #---------------------------------------------------------------------------------------\n",
    "#     w_ = params_dict['w'+str(n_hidden)]\n",
    "    \n",
    "#     #Applying regularization\n",
    "#     if regularization=='L1':\n",
    "#         reg_derivative = np.divide(w_, abs(w_), out=np.zeros_like(w_), where=abs(w_)!=0)\n",
    "#         reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n",
    "#         print('Regularization L1 :',reg_derivative.shape)\n",
    "                \n",
    "#     elif regularization=='L2':            \n",
    "#         reg_derivative = w_\n",
    "#         print('Regularization L2 :',reg_derivative.shape)\n",
    "    \n",
    "#     elif regularization==None:\n",
    "#         reg_derivative = 0\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    if activation_list[-1]=='sigmoid':\n",
    "        \n",
    "        #print('Last Layer y true shape :',y_true.shape)\n",
    "        #print('Last Layer y pred shape :',y_pred.shape)\n",
    "        \n",
    "        #grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred))) + (reg_lambda * reg_derivative)\n",
    "        grad_final_layer = -((y_true/y_pred) - ((1-y_true)/(1-y_pred)))\n",
    "        #print('Last Layer gradient shape :',grad_final_layer.shape)\n",
    "        \n",
    "    elif activation_list[-1]=='linear':\n",
    "        \n",
    "        #grad_final_layer = - 2 * (y_true-y_pred) + (reg_lambda * reg_derivative) #Check the sign\n",
    "        grad_final_layer = - 2 * (y_true-y_pred)\n",
    "        \n",
    "    return grad_final_layer\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "#Back=Propagation         \n",
    "def back_propagation(cache,params_dict,data_x,data_y,n_hidden,hidden_size_list,activation_list,y_pred,regularization,reg_lambda):\n",
    "    \n",
    "    grads_cache = {}\n",
    "    #db_cache = {}\n",
    "    \n",
    "    da = dL_last_layer(activation_list=activation_list,\n",
    "                             y_true=data_y.T,\n",
    "                             y_pred=y_pred,\n",
    "                             regularization=regularization,\n",
    "                             reg_lambda=reg_lambda,\n",
    "                             params_dict=params_dict,\n",
    "                             n_hidden=n_hidden)\n",
    "    #print('Final da shape :',da.shape)\n",
    "    \n",
    "    m = data_y.shape[0] #Data in the batches\n",
    "    \n",
    "    #print('dm in backprop :',m)\n",
    "    for layer_idx in list(reversed(range(1,n_hidden+1))):\n",
    "        \n",
    "        #print('# -------- Layer :',layer_idx,'-------- Size :',hidden_size_list[layer_idx],'--------#')\n",
    "        \n",
    "        activation_ = activation_list[layer_idx]\n",
    "        a = cache['a'+str(layer_idx)]\n",
    "        a_prev = cache['a'+str(layer_idx-1)]\n",
    "        w = params_dict['w'+str(layer_idx)]\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------\n",
    "        #Applying regularization\n",
    "        if regularization=='L1':\n",
    "            reg_derivative = np.divide(w, abs(w), out=np.zeros_like(w), where=abs(w)!=0)\n",
    "            reg_derivative = np.where(reg_derivative==np.inf,0,reg_derivative)\n",
    "            #print('Regularization L1 :',reg_derivative.shape)\n",
    "\n",
    "        elif regularization=='L2':            \n",
    "            reg_derivative = w\n",
    "            #print('Regularization L2 :',reg_derivative.shape)\n",
    "\n",
    "        elif regularization==None:\n",
    "            reg_derivative = 0\n",
    "        #_-------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "#         print('Shape of a:',a.shape)\n",
    "#         print('Shape of a_prev:',a_prev.shape)\n",
    "#         print('SHape of w:',w.shape)\n",
    "        \n",
    "        #z = \n",
    "        \n",
    "        dz =  da * (grad_fn_dz(activation=activation_,a=a))\n",
    "        \n",
    "        #print('dz shape :',dz.shape)\n",
    "                     \n",
    "        dw = (1/m) * (np.dot(dz, a_prev.T) + (reg_lambda * reg_derivative))\n",
    "        #print('dw shape :',dw.shape)\n",
    "        grads_cache['dw'+str(layer_idx)] = dw\n",
    "                     \n",
    "        db = (1/m) * np.sum(dz, axis=1,keepdims=True)\n",
    "        #print('db shape :',db.shape)\n",
    "        grads_cache['db'+str(layer_idx)] = db\n",
    "        \n",
    "        da = np.dot(w.T,dz)\n",
    "        #print('da shape :',da.shape)\n",
    "\n",
    "    return grads_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c36fe",
   "metadata": {
    "papermill": {
     "duration": 0.023393,
     "end_time": "2021-08-01T08:06:12.257134",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.233741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for updating weights through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a46204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.307912Z",
     "iopub.status.busy": "2021-08-01T08:06:12.307307Z",
     "iopub.status.idle": "2021-08-01T08:06:12.312049Z",
     "shell.execute_reply": "2021-08-01T08:06:12.312526Z",
     "shell.execute_reply.started": "2021-08-01T08:01:39.683025Z"
    },
    "papermill": {
     "duration": 0.031809,
     "end_time": "2021-08-01T08:06:12.312688",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.280879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_weights(params,grads_cache,alpha,n_hidden):\n",
    "    \n",
    "    for layer_idx in list(reversed(range(1,n_hidden+1))):\n",
    "        \n",
    "        #print('#---- layer :',layer_idx,'----#')\n",
    "        \n",
    "        dw = grads_cache['dw'+str(layer_idx)]\n",
    "        db = grads_cache['db'+str(layer_idx)]\n",
    "        \n",
    "#         print('dw shape :',dw.shape)\n",
    "#         print('db shape :',db.shape)\n",
    "#         print('w shape :',params['w'+str(layer_idx)].shape)\n",
    "        \n",
    "        params['w'+str(layer_idx)] -= alpha * dw\n",
    "        params['b'+str(layer_idx)] -= alpha * db\n",
    "\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e9ddd",
   "metadata": {
    "papermill": {
     "duration": 0.023816,
     "end_time": "2021-08-01T08:06:12.360890",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.337074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## UDF for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa0d8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.412647Z",
     "iopub.status.busy": "2021-08-01T08:06:12.411011Z",
     "iopub.status.idle": "2021-08-01T08:06:12.418089Z",
     "shell.execute_reply": "2021-08-01T08:06:12.417238Z",
     "shell.execute_reply.started": "2021-08-01T07:10:12.184009Z"
    },
    "papermill": {
     "duration": 0.033561,
     "end_time": "2021-08-01T08:06:12.418321",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.384760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(params,test_x,n_hidden,hidden_size_list,activation_list,threshold):\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    #Forward Propagation on trained weights\n",
    "    cache,y_pred = forward_propagation(params_dict=params,\n",
    "                                  data_x=test_x,\n",
    "                                  data_y=None,\n",
    "                                  n_hidden=n_hidden,\n",
    "                                  hidden_size_list=hidden_size_list,\n",
    "                                  activation_list=activation_list)\n",
    "    #print(cache)\n",
    "    preds = np.where(y_pred>threshold,1,0).astype(float)\n",
    "    return cache,np.round(y_pred,4),preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbdcd5",
   "metadata": {
    "papermill": {
     "duration": 0.028106,
     "end_time": "2021-08-01T08:06:12.479676",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.451570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stochastic Gradient Descent (SGD) for training of the ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba315c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.542518Z",
     "iopub.status.busy": "2021-08-01T08:06:12.541806Z",
     "iopub.status.idle": "2021-08-01T08:06:12.544614Z",
     "shell.execute_reply": "2021-08-01T08:06:12.545113Z",
     "shell.execute_reply.started": "2021-08-01T07:21:52.309836Z"
    },
    "papermill": {
     "duration": 0.040666,
     "end_time": "2021-08-01T08:06:12.545306",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.504640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ANN_train_sgd(data_x_overall,data_y_overall,batch_size,alpha,n_iters,n_hidden,hidden_size_list,activation_list,regularization,reg_lambda):\n",
    "    \n",
    "    print('Total training rows :',data_x_overall.shape[0])\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    #Creating x-y batches according to the provided batch_size\n",
    "    \n",
    "    n_batches = data_x_overall.shape[0]//batch_size\n",
    "    print('Total Batches to create in each epoch/iter :',n_batches)\n",
    "    \n",
    "    batches_x = np.array_split(data_x_overall,n_batches)\n",
    "    print('Total Batches of X:',len(batches_x))\n",
    "\n",
    "    batches_y = np.array_split(data_y_overall,n_batches)\n",
    "    print('Total Batches of y:',len(batches_y))\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    cost_history = [] #Record of cost through epochs\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    #Initialization of params\n",
    "    params_dict = generate_param_grid(a_prev=data_x_overall.T,\n",
    "                             n_hidden=n_hidden,\n",
    "                             hidden_size_list=hidden_size_list)\n",
    "    print('#----------------- Initial params ------------------#')\n",
    "    print(params_dict)\n",
    "    initial_params_abcd = params_dict.copy()\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    cache_tray = []\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "\n",
    "        if (epoch>0) & (epoch%100==0):\n",
    "            print('#----------------------------------- Epoch :',epoch,'--------------------------------------#')\n",
    "            print('cost :',cost)\n",
    "            \n",
    "        for j in range(len(batches_x)): #For each batch created for each epoch/iter\n",
    "            \n",
    "            #-------------------------------------------------------------------------\n",
    "            #For each batch of data\n",
    "            data_x = batches_x[j]\n",
    "            data_y = batches_y[j]\n",
    "\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Forward Propagation\n",
    "            cache,y_pred = forward_propagation(params_dict=params_dict,\n",
    "                                          data_x=data_x,\n",
    "                                          data_y=data_y,\n",
    "                                          n_hidden=n_hidden,\n",
    "                                          hidden_size_list=hidden_size_list,\n",
    "                                          activation_list=activation_list)\n",
    "            #print(np.max(y_pred))\n",
    "            #cache_tray.append(cache)\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Cost calculation\n",
    "            cost = cost_calculation(activation_list=activation_list,\n",
    "                             y_true=data_y.T,\n",
    "                             y_pred=y_pred,\n",
    "                             regularization=regularization,\n",
    "                             reg_lambda=reg_lambda,\n",
    "                             params_dict=params_dict,\n",
    "                             n_hidden=n_hidden)\n",
    "\n",
    "            #cost_history.append(cost)\n",
    "            #print('cost :',cost)\n",
    "\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Back Propagation\n",
    "            grads_cache_ = back_propagation(cache=cache,\n",
    "                                           params_dict=params_dict,\n",
    "                                           data_x=data_x,\n",
    "                                           data_y=data_y,\n",
    "                                           n_hidden=n_hidden,\n",
    "                                           hidden_size_list=hidden_size_list,\n",
    "                                           activation_list=activation_list,\n",
    "                                           y_pred=y_pred,\n",
    "                                           regularization=regularization,\n",
    "                                           reg_lambda=reg_lambda)\n",
    "\n",
    "            #------------------------------------------------------------------------\n",
    "            #Updating weights\n",
    "            params_dict = update_weights(params=params_dict,\n",
    "                                         grads_cache=grads_cache_,\n",
    "                                         alpha=alpha,\n",
    "                                         n_hidden=n_hidden)\n",
    "            \n",
    "        cost_history.append(cost) #Appending cost after each epoch\n",
    "\n",
    "\n",
    "    return initial_params_abcd,params_dict,grads_cache_,cost_history,y_pred,cache_tray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccddd6b",
   "metadata": {
    "papermill": {
     "duration": 0.024601,
     "end_time": "2021-08-01T08:06:12.593887",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.569286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model by invoking the above UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "562a48ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:12.644563Z",
     "iopub.status.busy": "2021-08-01T08:06:12.643929Z",
     "iopub.status.idle": "2021-08-01T08:06:15.345760Z",
     "shell.execute_reply": "2021-08-01T08:06:15.346225Z",
     "shell.execute_reply.started": "2021-08-01T08:01:56.828870Z"
    },
    "papermill": {
     "duration": 2.728803,
     "end_time": "2021-08-01T08:06:15.346412",
     "exception": false,
     "start_time": "2021-08-01T08:06:12.617609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rows : 792\n",
      "Total Batches to create in each epoch/iter : 31\n",
      "Total Batches of X: 31\n",
      "Total Batches of y: 31\n",
      "#----------------- Initial params ------------------#\n",
      "{'w1': array([[0.0567285 , 0.03637208, 0.04504087, 0.03882087, 0.00082978,\n",
      "        0.09497138, 0.07198071, 0.09863682, 0.03548203, 0.02435965,\n",
      "        0.06428808],\n",
      "       [0.06415114, 0.04942527, 0.04817833, 0.05184479, 0.04402826,\n",
      "        0.05264394, 0.08115342, 0.09137596, 0.04938978, 0.04425654,\n",
      "        0.0899093 ],\n",
      "       [0.08605914, 0.01660075, 0.09961426, 0.01690996, 0.09081057,\n",
      "        0.09463972, 0.00463481, 0.05335758, 0.06594618, 0.0926608 ,\n",
      "        0.06901948]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'w2': array([[0.03059515, 0.02233453, 0.09508021]]), 'b2': array([[0.]])}\n",
      "#----------------------------------- Epoch : 100 --------------------------------------#\n",
      "cost : 0.5918014308469565\n",
      "#----------------------------------- Epoch : 200 --------------------------------------#\n",
      "cost : 0.45912719199435986\n",
      "#----------------------------------- Epoch : 300 --------------------------------------#\n",
      "cost : 0.4005252200163516\n",
      "#----------------------------------- Epoch : 400 --------------------------------------#\n",
      "cost : 0.37678453026091197\n",
      "#----------------------------------- Epoch : 500 --------------------------------------#\n",
      "cost : 0.3681325463422393\n"
     ]
    }
   ],
   "source": [
    "#Defining hyper-parameters for ANN\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "n_hidden = 2       #No of hidden layers\n",
    "alpha = 0.003      #Learning_rate\n",
    "n_iters = 501      #Total epochs\n",
    "hidden_size_list = [0,3,1]               #first element will be 0 and not counted in hidden layers\n",
    "activation_list = [0,'relu','sigmoid']   #first element will be 0 and not counted in hidden layers\n",
    "batch_size = 25    #Batch wise gradient descent\n",
    "regularization = 'L1'\n",
    "reg_lambda = 0.09\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "initial_params_train,params_dict_train,grads,cost_history_train,y_pred_train,cache_tray = ANN_train_sgd(data_x_overall=X_arr,\n",
    "                                                                                                       data_y_overall=y_arr,\n",
    "                                                                                                       batch_size=batch_size,\n",
    "                                                                                                       alpha=alpha,\n",
    "                                                                                                       n_iters=n_iters,\n",
    "                                                                                                       n_hidden=n_hidden,\n",
    "                                                                                                       hidden_size_list=hidden_size_list,\n",
    "                                                                                                       activation_list=activation_list,\n",
    "                                                                                                       regularization=regularization,\n",
    "                                                                                                       reg_lambda=reg_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660f9a6",
   "metadata": {
    "papermill": {
     "duration": 0.026178,
     "end_time": "2021-08-01T08:06:15.412027",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.385849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cost-Epoch plot for the manual ANN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b0b579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:15.494077Z",
     "iopub.status.busy": "2021-08-01T08:06:15.491573Z",
     "iopub.status.idle": "2021-08-01T08:06:15.702191Z",
     "shell.execute_reply": "2021-08-01T08:06:15.701743Z",
     "shell.execute_reply.started": "2021-08-01T08:02:04.042683Z"
    },
    "papermill": {
     "duration": 0.265261,
     "end_time": "2021-08-01T08:06:15.702364",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.437103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'epochs'),\n",
       " Text(0, 0.5, 'cost'),\n",
       " Text(0.5, 1.0, 'Cost vs epoch plot for Manual ANN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cost plot over epochs (1 value at end of each epoch) - over the last batch\n",
    "ax = sns.lineplot(x=list(range(n_iters)),y=cost_history_train)\n",
    "ax.set(xlabel='epochs',ylabel='cost',title='Cost vs epoch plot for Manual ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e155f",
   "metadata": {
    "papermill": {
     "duration": 0.025755,
     "end_time": "2021-08-01T08:06:15.754912",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.729157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d49a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:15.814378Z",
     "iopub.status.busy": "2021-08-01T08:06:15.813739Z",
     "iopub.status.idle": "2021-08-01T08:06:15.821620Z",
     "shell.execute_reply": "2021-08-01T08:06:15.822231Z",
     "shell.execute_reply.started": "2021-08-01T08:02:07.412601Z"
    },
    "papermill": {
     "duration": 0.041334,
     "end_time": "2021-08-01T08:06:15.822475",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.781141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prediction array : (1, 100)\n",
      "Unique predictions : [0. 1.]\n",
      "Unique of predict proba : [0.1936 0.1939 0.2026 0.2147 0.2434 0.2469 0.2482 0.2486 0.2752 0.2807\n",
      " 0.2984 0.3707 0.3854 0.4093 0.437  0.442  0.4523 0.4992 0.4999 0.5269\n",
      " 0.5291 0.6975 0.7177 0.7409 0.8149 0.8421 0.8892 0.9262 0.9671 0.968\n",
      " 0.9753 0.9785 0.979  0.98   0.981  0.9861 0.9897 0.9913 0.9955 0.9962\n",
      " 0.9964 0.9975 0.9978] \n",
      "\n",
      "#--------------------- Evaluation ----------------------#\n",
      "ROC AUC of test set : 0.7899305555555556\n",
      "Accuracy of test set : 0.84\n"
     ]
    }
   ],
   "source": [
    "cache,preds_proba,manual_preds = prediction(params=params_dict_train,\n",
    "                                            test_x=X_test_arr,\n",
    "                                            n_hidden=n_hidden,\n",
    "                                            hidden_size_list=hidden_size_list,\n",
    "                                            activation_list=activation_list,\n",
    "                                            threshold=0.5)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "print('Shape of prediction array :',preds_proba.shape)\n",
    "print('Unique predictions :',np.unique(manual_preds))\n",
    "print('Unique of predict proba :',np.unique(preds_proba),'\\n')\n",
    "\n",
    "print('#--------------------- Evaluation ----------------------#')\n",
    "#Evaluation of the predictions\n",
    "print('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),manual_preds.ravel()))\n",
    "print('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),manual_preds.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f72002",
   "metadata": {
    "papermill": {
     "duration": 0.026398,
     "end_time": "2021-08-01T08:06:15.875890",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.849492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking with Keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495dc79",
   "metadata": {
    "papermill": {
     "duration": 0.026578,
     "end_time": "2021-08-01T08:06:15.929333",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.902755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40d9fb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:15.987562Z",
     "iopub.status.busy": "2021-08-01T08:06:15.986880Z",
     "iopub.status.idle": "2021-08-01T08:06:21.159483Z",
     "shell.execute_reply": "2021-08-01T08:06:21.158951Z",
     "shell.execute_reply.started": "2021-08-01T07:45:16.667986Z"
    },
    "papermill": {
     "duration": 5.203922,
     "end_time": "2021-08-01T08:06:21.159634",
     "exception": false,
     "start_time": "2021-08-01T08:06:15.955712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow.keras.models\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6a7d5",
   "metadata": {
    "papermill": {
     "duration": 0.025942,
     "end_time": "2021-08-01T08:06:21.212290",
     "exception": false,
     "start_time": "2021-08-01T08:06:21.186348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining the model with same specifications as manual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef58c64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:21.272948Z",
     "iopub.status.busy": "2021-08-01T08:06:21.272229Z",
     "iopub.status.idle": "2021-08-01T08:06:21.536872Z",
     "shell.execute_reply": "2021-08-01T08:06:21.536374Z",
     "shell.execute_reply.started": "2021-08-01T07:45:44.449853Z"
    },
    "papermill": {
     "duration": 0.298576,
     "end_time": "2021-08-01T08:06:21.537011",
     "exception": false,
     "start_time": "2021-08-01T08:06:21.238435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization applied : L1 -: 0.09\n",
      "Model: \"ANN_keras\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 11)]              0         \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 3)                 36        \n",
      "_________________________________________________________________\n",
      "pred (Dense)                 (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 40\n",
      "Trainable params: 40\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining regularization application to the keras functional API\n",
    "if regularization=='L1':\n",
    "    print('Regularization applied :',regularization,'-:',reg_lambda)\n",
    "    reg_keras = tf.keras.regularizers.l1(reg_lambda)\n",
    "elif regularization=='L2':\n",
    "    print('Regularization applied :',regularization)\n",
    "    reg_keras = tf.keras.regularizers.l2(reg_lambda)\n",
    "elif regularization==None:\n",
    "    print('Regularization applied :',regularization)\n",
    "    reg_keras = None\n",
    "\n",
    "def ANN_keras(x):\n",
    "    \n",
    "    input_ = tfl.Input(shape=(x.shape[1],))\n",
    "    \n",
    "    x = tfl.Dense(3,activation='relu', kernel_regularizer=reg_keras,name = 'Dense_3')(input_) #Layer 1\n",
    "    \n",
    "    preds = tfl.Dense(1, activation=\"sigmoid\", name=\"pred\")(x) #Output layer\n",
    "    \n",
    "    model = Model(input_, preds, name=\"ANN_keras\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=alpha)) #Stochastic Gradient Descent with specified alpha\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = ANN_keras(X_arr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006faba9",
   "metadata": {
    "papermill": {
     "duration": 0.026772,
     "end_time": "2021-08-01T08:06:21.590721",
     "exception": false,
     "start_time": "2021-08-01T08:06:21.563949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954c498a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:06:21.649298Z",
     "iopub.status.busy": "2021-08-01T08:06:21.648645Z",
     "iopub.status.idle": "2021-08-01T08:07:01.662751Z",
     "shell.execute_reply": "2021-08-01T08:07:01.663245Z",
     "shell.execute_reply.started": "2021-08-01T07:45:47.967991Z"
    },
    "papermill": {
     "duration": 40.045785,
     "end_time": "2021-08-01T08:07:01.663456",
     "exception": false,
     "start_time": "2021-08-01T08:06:21.617671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/501\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 1.7302 - val_loss: 1.6734\n",
      "Epoch 2/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.6993 - val_loss: 1.6460\n",
      "Epoch 3/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6647 - val_loss: 1.6190\n",
      "Epoch 4/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6351 - val_loss: 1.5926\n",
      "Epoch 5/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6182 - val_loss: 1.5661\n",
      "Epoch 6/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5841 - val_loss: 1.5397\n",
      "Epoch 7/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5528 - val_loss: 1.5133\n",
      "Epoch 8/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5223 - val_loss: 1.4875\n",
      "Epoch 9/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4999 - val_loss: 1.4630\n",
      "Epoch 10/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4887 - val_loss: 1.4388\n",
      "Epoch 11/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4568 - val_loss: 1.4148\n",
      "Epoch 12/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4375 - val_loss: 1.3911\n",
      "Epoch 13/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.4088 - val_loss: 1.3673\n",
      "Epoch 14/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3920 - val_loss: 1.3435\n",
      "Epoch 15/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3642 - val_loss: 1.3216\n",
      "Epoch 16/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3447 - val_loss: 1.3003\n",
      "Epoch 17/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3212 - val_loss: 1.2789\n",
      "Epoch 18/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2970 - val_loss: 1.2576\n",
      "Epoch 19/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2745 - val_loss: 1.2361\n",
      "Epoch 20/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2584 - val_loss: 1.2147\n",
      "Epoch 21/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2327 - val_loss: 1.1939\n",
      "Epoch 22/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2246 - val_loss: 1.1733\n",
      "Epoch 23/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1963 - val_loss: 1.1529\n",
      "Epoch 24/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1741 - val_loss: 1.1327\n",
      "Epoch 25/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1534 - val_loss: 1.1142\n",
      "Epoch 26/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1326 - val_loss: 1.0973\n",
      "Epoch 27/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1148 - val_loss: 1.0805\n",
      "Epoch 28/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0960 - val_loss: 1.0637\n",
      "Epoch 29/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0919 - val_loss: 1.0468\n",
      "Epoch 30/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0706 - val_loss: 1.0300\n",
      "Epoch 31/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0626 - val_loss: 1.0134\n",
      "Epoch 32/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0409 - val_loss: 0.9967\n",
      "Epoch 33/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0238 - val_loss: 0.9816\n",
      "Epoch 34/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0058 - val_loss: 0.9666\n",
      "Epoch 35/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9963 - val_loss: 0.9518\n",
      "Epoch 36/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9736 - val_loss: 0.9380\n",
      "Epoch 37/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9669 - val_loss: 0.9244\n",
      "Epoch 38/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9456 - val_loss: 0.9111\n",
      "Epoch 39/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.9301 - val_loss: 0.8977\n",
      "Epoch 40/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.8844\n",
      "Epoch 41/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9093 - val_loss: 0.8719\n",
      "Epoch 42/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8948 - val_loss: 0.8594\n",
      "Epoch 43/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8786 - val_loss: 0.8477\n",
      "Epoch 44/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8734 - val_loss: 0.8366\n",
      "Epoch 45/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8662 - val_loss: 0.8252\n",
      "Epoch 46/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8550 - val_loss: 0.8141\n",
      "Epoch 47/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8339 - val_loss: 0.8031\n",
      "Epoch 48/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8207 - val_loss: 0.7926\n",
      "Epoch 49/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8045 - val_loss: 0.7829\n",
      "Epoch 50/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7999 - val_loss: 0.7733\n",
      "Epoch 51/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7851 - val_loss: 0.7638\n",
      "Epoch 52/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7845 - val_loss: 0.7543\n",
      "Epoch 53/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7688 - val_loss: 0.7449\n",
      "Epoch 54/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7619 - val_loss: 0.7363\n",
      "Epoch 55/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7465 - val_loss: 0.7279\n",
      "Epoch 56/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7435 - val_loss: 0.7209\n",
      "Epoch 57/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7303 - val_loss: 0.7154\n",
      "Epoch 58/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7284 - val_loss: 0.7103\n",
      "Epoch 59/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7182 - val_loss: 0.7052\n",
      "Epoch 60/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7261 - val_loss: 0.7007\n",
      "Epoch 61/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7107 - val_loss: 0.6967\n",
      "Epoch 62/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7149 - val_loss: 0.6923\n",
      "Epoch 63/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7132 - val_loss: 0.6882\n",
      "Epoch 64/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6978 - val_loss: 0.6841\n",
      "Epoch 65/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6945 - val_loss: 0.6808\n",
      "Epoch 66/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6918 - val_loss: 0.6775\n",
      "Epoch 67/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6944 - val_loss: 0.6742\n",
      "Epoch 68/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6794 - val_loss: 0.6709\n",
      "Epoch 69/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6869 - val_loss: 0.6677\n",
      "Epoch 70/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6824 - val_loss: 0.6650\n",
      "Epoch 71/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.6626\n",
      "Epoch 72/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6779 - val_loss: 0.6601\n",
      "Epoch 73/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6678 - val_loss: 0.6580\n",
      "Epoch 74/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6640 - val_loss: 0.6569\n",
      "Epoch 75/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6564 - val_loss: 0.6558\n",
      "Epoch 76/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6634 - val_loss: 0.6553\n",
      "Epoch 77/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.6548\n",
      "Epoch 78/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6656 - val_loss: 0.6545\n",
      "Epoch 79/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6737 - val_loss: 0.6539\n",
      "Epoch 80/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6658 - val_loss: 0.6535\n",
      "Epoch 81/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.6530\n",
      "Epoch 82/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6579 - val_loss: 0.6526\n",
      "Epoch 83/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6610 - val_loss: 0.6524\n",
      "Epoch 84/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6654 - val_loss: 0.6518\n",
      "Epoch 85/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.6514\n",
      "Epoch 86/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6710 - val_loss: 0.6508\n",
      "Epoch 87/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6628 - val_loss: 0.6506\n",
      "Epoch 88/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6724 - val_loss: 0.6500\n",
      "Epoch 89/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6575 - val_loss: 0.6497\n",
      "Epoch 90/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6749 - val_loss: 0.6492\n",
      "Epoch 91/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6559 - val_loss: 0.6488\n",
      "Epoch 92/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.6485\n",
      "Epoch 93/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.6482\n",
      "Epoch 94/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6599 - val_loss: 0.6478\n",
      "Epoch 95/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6438 - val_loss: 0.6475\n",
      "Epoch 96/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6470\n",
      "Epoch 97/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.6465\n",
      "Epoch 98/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6430 - val_loss: 0.6463\n",
      "Epoch 99/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6532 - val_loss: 0.6459\n",
      "Epoch 100/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6676 - val_loss: 0.6453\n",
      "Epoch 101/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.6451\n",
      "Epoch 102/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6460 - val_loss: 0.6448\n",
      "Epoch 103/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.6445\n",
      "Epoch 104/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6636 - val_loss: 0.6440\n",
      "Epoch 105/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6321 - val_loss: 0.6437\n",
      "Epoch 106/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6396 - val_loss: 0.6433\n",
      "Epoch 107/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.6429\n",
      "Epoch 108/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6533 - val_loss: 0.6426\n",
      "Epoch 109/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6479 - val_loss: 0.6424\n",
      "Epoch 110/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6475 - val_loss: 0.6420\n",
      "Epoch 111/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6417\n",
      "Epoch 112/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.6414\n",
      "Epoch 113/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6518 - val_loss: 0.6413\n",
      "Epoch 114/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6506 - val_loss: 0.6406\n",
      "Epoch 115/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6443 - val_loss: 0.6404\n",
      "Epoch 116/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6412 - val_loss: 0.6400\n",
      "Epoch 117/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6576 - val_loss: 0.6398\n",
      "Epoch 118/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6403 - val_loss: 0.6394\n",
      "Epoch 119/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6377 - val_loss: 0.6389\n",
      "Epoch 120/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6501 - val_loss: 0.6385\n",
      "Epoch 121/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6534 - val_loss: 0.6383\n",
      "Epoch 122/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6495 - val_loss: 0.6379\n",
      "Epoch 123/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6359 - val_loss: 0.6380\n",
      "Epoch 124/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6385 - val_loss: 0.6374\n",
      "Epoch 125/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6374 - val_loss: 0.6369\n",
      "Epoch 126/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6399 - val_loss: 0.6368\n",
      "Epoch 127/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6443 - val_loss: 0.6363\n",
      "Epoch 128/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6454 - val_loss: 0.6362\n",
      "Epoch 129/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6398 - val_loss: 0.6356\n",
      "Epoch 130/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.6354\n",
      "Epoch 131/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 0.6352\n",
      "Epoch 132/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.6348\n",
      "Epoch 133/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.6345\n",
      "Epoch 134/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6406 - val_loss: 0.6342\n",
      "Epoch 135/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6515 - val_loss: 0.6338\n",
      "Epoch 136/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6404 - val_loss: 0.6336\n",
      "Epoch 137/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.6332\n",
      "Epoch 138/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6436 - val_loss: 0.6329\n",
      "Epoch 139/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6442 - val_loss: 0.6326\n",
      "Epoch 140/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.6322\n",
      "Epoch 141/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.6319\n",
      "Epoch 142/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6404 - val_loss: 0.6316\n",
      "Epoch 143/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6266 - val_loss: 0.6315\n",
      "Epoch 144/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6242 - val_loss: 0.6310\n",
      "Epoch 145/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6374 - val_loss: 0.6310\n",
      "Epoch 146/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6289 - val_loss: 0.6304\n",
      "Epoch 147/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6385 - val_loss: 0.6303\n",
      "Epoch 148/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6481 - val_loss: 0.6299\n",
      "Epoch 149/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6311 - val_loss: 0.6295\n",
      "Epoch 150/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6418 - val_loss: 0.6293\n",
      "Epoch 151/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.6290\n",
      "Epoch 152/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6417 - val_loss: 0.6286\n",
      "Epoch 153/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6403 - val_loss: 0.6288\n",
      "Epoch 154/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6279\n",
      "Epoch 155/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6277\n",
      "Epoch 156/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6348 - val_loss: 0.6274\n",
      "Epoch 157/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6333 - val_loss: 0.6272\n",
      "Epoch 158/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6356 - val_loss: 0.6269\n",
      "Epoch 159/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6524 - val_loss: 0.6266\n",
      "Epoch 160/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6315 - val_loss: 0.6263\n",
      "Epoch 161/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6204 - val_loss: 0.6259\n",
      "Epoch 162/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6290 - val_loss: 0.6255\n",
      "Epoch 163/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6429 - val_loss: 0.6252\n",
      "Epoch 164/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6461 - val_loss: 0.6252\n",
      "Epoch 165/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6309 - val_loss: 0.6249\n",
      "Epoch 166/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6275 - val_loss: 0.6245\n",
      "Epoch 167/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6265 - val_loss: 0.6246\n",
      "Epoch 168/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6188 - val_loss: 0.6240\n",
      "Epoch 169/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6417 - val_loss: 0.6237\n",
      "Epoch 170/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6272 - val_loss: 0.6233\n",
      "Epoch 171/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6317 - val_loss: 0.6232\n",
      "Epoch 172/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6229 - val_loss: 0.6227\n",
      "Epoch 173/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6404 - val_loss: 0.6226\n",
      "Epoch 174/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.6222\n",
      "Epoch 175/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6170 - val_loss: 0.6219\n",
      "Epoch 176/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6221 - val_loss: 0.6218\n",
      "Epoch 177/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6345 - val_loss: 0.6215\n",
      "Epoch 178/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6275 - val_loss: 0.6213\n",
      "Epoch 179/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6057 - val_loss: 0.6211\n",
      "Epoch 180/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6361 - val_loss: 0.6206\n",
      "Epoch 181/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6293 - val_loss: 0.6205\n",
      "Epoch 182/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6182 - val_loss: 0.6200\n",
      "Epoch 183/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6336 - val_loss: 0.6201\n",
      "Epoch 184/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 0.6195\n",
      "Epoch 185/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6351 - val_loss: 0.6193\n",
      "Epoch 186/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6294 - val_loss: 0.6189\n",
      "Epoch 187/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6181 - val_loss: 0.6187\n",
      "Epoch 188/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6287 - val_loss: 0.6184\n",
      "Epoch 189/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6225 - val_loss: 0.6183\n",
      "Epoch 190/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6237 - val_loss: 0.6181\n",
      "Epoch 191/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6327 - val_loss: 0.6177\n",
      "Epoch 192/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6228 - val_loss: 0.6174\n",
      "Epoch 193/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6333 - val_loss: 0.6171\n",
      "Epoch 194/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5996 - val_loss: 0.6170\n",
      "Epoch 195/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.6166\n",
      "Epoch 196/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6053 - val_loss: 0.6164\n",
      "Epoch 197/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6181 - val_loss: 0.6161\n",
      "Epoch 198/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6220 - val_loss: 0.6159\n",
      "Epoch 199/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 0.6157\n",
      "Epoch 200/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.6155\n",
      "Epoch 201/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6366 - val_loss: 0.6155\n",
      "Epoch 202/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6262 - val_loss: 0.6151\n",
      "Epoch 203/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6215 - val_loss: 0.6147\n",
      "Epoch 204/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 0.6145\n",
      "Epoch 205/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6305 - val_loss: 0.6143\n",
      "Epoch 206/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6344 - val_loss: 0.6141\n",
      "Epoch 207/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6075 - val_loss: 0.6138\n",
      "Epoch 208/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6282 - val_loss: 0.6135\n",
      "Epoch 209/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6178 - val_loss: 0.6131\n",
      "Epoch 210/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 0.6129\n",
      "Epoch 211/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5864 - val_loss: 0.6129\n",
      "Epoch 212/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 0.6125\n",
      "Epoch 213/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6152 - val_loss: 0.6121\n",
      "Epoch 214/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6350 - val_loss: 0.6120\n",
      "Epoch 215/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6363 - val_loss: 0.6119\n",
      "Epoch 216/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6167 - val_loss: 0.6117\n",
      "Epoch 217/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6251 - val_loss: 0.6113\n",
      "Epoch 218/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.6112\n",
      "Epoch 219/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6172 - val_loss: 0.6111\n",
      "Epoch 220/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6159 - val_loss: 0.6107\n",
      "Epoch 221/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5940 - val_loss: 0.6105\n",
      "Epoch 222/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6200 - val_loss: 0.6101\n",
      "Epoch 223/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.6104\n",
      "Epoch 224/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 0.6099\n",
      "Epoch 225/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6200 - val_loss: 0.6096\n",
      "Epoch 226/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.6092\n",
      "Epoch 227/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.6091\n",
      "Epoch 228/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.6087\n",
      "Epoch 229/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6155 - val_loss: 0.6089\n",
      "Epoch 230/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6230 - val_loss: 0.6083\n",
      "Epoch 231/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6228 - val_loss: 0.6082\n",
      "Epoch 232/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 0.6081\n",
      "Epoch 233/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5978 - val_loss: 0.6076\n",
      "Epoch 234/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.6073\n",
      "Epoch 235/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 0.6072\n",
      "Epoch 236/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6239 - val_loss: 0.6071\n",
      "Epoch 237/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5979 - val_loss: 0.6068\n",
      "Epoch 238/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.6066\n",
      "Epoch 239/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.6062\n",
      "Epoch 240/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6087 - val_loss: 0.6063\n",
      "Epoch 241/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6010 - val_loss: 0.6059\n",
      "Epoch 242/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6162 - val_loss: 0.6056\n",
      "Epoch 243/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 0.6057\n",
      "Epoch 244/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6028 - val_loss: 0.6054\n",
      "Epoch 245/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5943 - val_loss: 0.6050\n",
      "Epoch 246/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 0.6052\n",
      "Epoch 247/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.6045\n",
      "Epoch 248/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 0.6043\n",
      "Epoch 249/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.6041\n",
      "Epoch 250/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6162 - val_loss: 0.6037\n",
      "Epoch 251/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.6036\n",
      "Epoch 252/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6227 - val_loss: 0.6039\n",
      "Epoch 253/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.6031\n",
      "Epoch 254/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6053 - val_loss: 0.6033\n",
      "Epoch 255/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6161 - val_loss: 0.6028\n",
      "Epoch 256/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6149 - val_loss: 0.6025\n",
      "Epoch 257/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.6023\n",
      "Epoch 258/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.6021\n",
      "Epoch 259/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5942 - val_loss: 0.6017\n",
      "Epoch 260/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6103 - val_loss: 0.6014\n",
      "Epoch 261/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.6013\n",
      "Epoch 262/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.6013\n",
      "Epoch 263/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.6009\n",
      "Epoch 264/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6094 - val_loss: 0.6007\n",
      "Epoch 265/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5759 - val_loss: 0.6005\n",
      "Epoch 266/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6184 - val_loss: 0.6004\n",
      "Epoch 267/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 0.5999\n",
      "Epoch 268/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.5998\n",
      "Epoch 269/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6155 - val_loss: 0.5996\n",
      "Epoch 270/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5993\n",
      "Epoch 271/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.5991\n",
      "Epoch 272/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.5988\n",
      "Epoch 273/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.5987\n",
      "Epoch 274/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 0.5983\n",
      "Epoch 275/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5962 - val_loss: 0.5984\n",
      "Epoch 276/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6133 - val_loss: 0.5977\n",
      "Epoch 277/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5977\n",
      "Epoch 278/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5959 - val_loss: 0.5973\n",
      "Epoch 279/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.5972\n",
      "Epoch 280/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6178 - val_loss: 0.5971\n",
      "Epoch 281/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.5967\n",
      "Epoch 282/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5964\n",
      "Epoch 283/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6127 - val_loss: 0.5962\n",
      "Epoch 284/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 0.5960\n",
      "Epoch 285/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.5960\n",
      "Epoch 286/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5957\n",
      "Epoch 287/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5954\n",
      "Epoch 288/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6124 - val_loss: 0.5955\n",
      "Epoch 289/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.5949\n",
      "Epoch 290/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.5947\n",
      "Epoch 291/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.5943\n",
      "Epoch 292/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5923 - val_loss: 0.5942\n",
      "Epoch 293/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6182 - val_loss: 0.5940\n",
      "Epoch 294/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 0.5937\n",
      "Epoch 295/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5917 - val_loss: 0.5935\n",
      "Epoch 296/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5934\n",
      "Epoch 297/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.5931\n",
      "Epoch 298/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6041 - val_loss: 0.5929\n",
      "Epoch 299/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.5928\n",
      "Epoch 300/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.5925\n",
      "Epoch 301/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5921\n",
      "Epoch 302/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5905 - val_loss: 0.5922\n",
      "Epoch 303/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5960 - val_loss: 0.5917\n",
      "Epoch 304/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5967 - val_loss: 0.5919\n",
      "Epoch 305/501\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6046 - val_loss: 0.5915\n",
      "Epoch 306/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6014 - val_loss: 0.5910\n",
      "Epoch 307/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6046 - val_loss: 0.5909\n",
      "Epoch 308/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 0.5907\n",
      "Epoch 309/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.5905\n",
      "Epoch 310/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 0.5903\n",
      "Epoch 311/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 0.5901\n",
      "Epoch 312/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.5899\n",
      "Epoch 313/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 0.5895\n",
      "Epoch 314/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.5895\n",
      "Epoch 315/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.5890\n",
      "Epoch 316/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5889\n",
      "Epoch 317/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5908 - val_loss: 0.5888\n",
      "Epoch 318/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6003 - val_loss: 0.5888\n",
      "Epoch 319/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5984 - val_loss: 0.5884\n",
      "Epoch 320/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5881\n",
      "Epoch 321/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 0.5879\n",
      "Epoch 322/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5948 - val_loss: 0.5878\n",
      "Epoch 323/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5984 - val_loss: 0.5874\n",
      "Epoch 324/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5769 - val_loss: 0.5872\n",
      "Epoch 325/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5655 - val_loss: 0.5868\n",
      "Epoch 326/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5870\n",
      "Epoch 327/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5864\n",
      "Epoch 328/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6179 - val_loss: 0.5868\n",
      "Epoch 329/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5721 - val_loss: 0.5861\n",
      "Epoch 330/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.5859\n",
      "Epoch 331/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6117 - val_loss: 0.5857\n",
      "Epoch 332/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 0.5856\n",
      "Epoch 333/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5962 - val_loss: 0.5856\n",
      "Epoch 334/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6078 - val_loss: 0.5849\n",
      "Epoch 335/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.5854\n",
      "Epoch 336/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.5846\n",
      "Epoch 337/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.5847\n",
      "Epoch 338/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 0.5842\n",
      "Epoch 339/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.5840\n",
      "Epoch 340/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 0.5842\n",
      "Epoch 341/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5642 - val_loss: 0.5840\n",
      "Epoch 342/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.5836\n",
      "Epoch 343/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6006 - val_loss: 0.5837\n",
      "Epoch 344/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5833\n",
      "Epoch 345/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5829\n",
      "Epoch 346/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5985 - val_loss: 0.5829\n",
      "Epoch 347/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5827\n",
      "Epoch 348/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5694 - val_loss: 0.5821\n",
      "Epoch 349/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.5823\n",
      "Epoch 350/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.5821\n",
      "Epoch 351/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.5818\n",
      "Epoch 352/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5817\n",
      "Epoch 353/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 0.5815\n",
      "Epoch 354/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.5810\n",
      "Epoch 355/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 0.5809\n",
      "Epoch 356/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5807\n",
      "Epoch 357/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.5805\n",
      "Epoch 358/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5732 - val_loss: 0.5806\n",
      "Epoch 359/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 0.5803\n",
      "Epoch 360/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5642 - val_loss: 0.5799\n",
      "Epoch 361/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5800\n",
      "Epoch 362/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5795\n",
      "Epoch 363/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5795\n",
      "Epoch 364/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.5791\n",
      "Epoch 365/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5790\n",
      "Epoch 366/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5687 - val_loss: 0.5790\n",
      "Epoch 367/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.5787\n",
      "Epoch 368/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5590 - val_loss: 0.5785\n",
      "Epoch 369/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 0.5784\n",
      "Epoch 370/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5783\n",
      "Epoch 371/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 0.5777\n",
      "Epoch 372/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5787\n",
      "Epoch 373/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6037 - val_loss: 0.5779\n",
      "Epoch 374/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 0.5775\n",
      "Epoch 375/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5600 - val_loss: 0.5773\n",
      "Epoch 376/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5771\n",
      "Epoch 377/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.5768\n",
      "Epoch 378/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.5769\n",
      "Epoch 379/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5765\n",
      "Epoch 380/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5831 - val_loss: 0.5762\n",
      "Epoch 381/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5682 - val_loss: 0.5759\n",
      "Epoch 382/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5759\n",
      "Epoch 383/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5763\n",
      "Epoch 384/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.5760\n",
      "Epoch 385/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5816 - val_loss: 0.5755\n",
      "Epoch 386/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5751\n",
      "Epoch 387/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5751\n",
      "Epoch 388/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5970 - val_loss: 0.5750\n",
      "Epoch 389/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5750\n",
      "Epoch 390/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5762 - val_loss: 0.5747\n",
      "Epoch 391/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 0.5745\n",
      "Epoch 392/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5673 - val_loss: 0.5743\n",
      "Epoch 393/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5740\n",
      "Epoch 394/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 0.5736\n",
      "Epoch 395/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5742\n",
      "Epoch 396/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5698 - val_loss: 0.5737\n",
      "Epoch 397/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.5735\n",
      "Epoch 398/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5737\n",
      "Epoch 399/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5587 - val_loss: 0.5731\n",
      "Epoch 400/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5727\n",
      "Epoch 401/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5510 - val_loss: 0.5732\n",
      "Epoch 402/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5731\n",
      "Epoch 403/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5708 - val_loss: 0.5722\n",
      "Epoch 404/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5725\n",
      "Epoch 405/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.5724\n",
      "Epoch 406/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5717\n",
      "Epoch 407/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 0.5716\n",
      "Epoch 408/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5769 - val_loss: 0.5717\n",
      "Epoch 409/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5729 - val_loss: 0.5716\n",
      "Epoch 410/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 0.5716\n",
      "Epoch 411/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5715\n",
      "Epoch 412/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5598 - val_loss: 0.5710\n",
      "Epoch 413/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5774 - val_loss: 0.5711\n",
      "Epoch 414/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.5708\n",
      "Epoch 415/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 0.5705\n",
      "Epoch 416/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5543 - val_loss: 0.5701\n",
      "Epoch 417/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5701\n",
      "Epoch 418/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5628 - val_loss: 0.5701\n",
      "Epoch 419/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6073 - val_loss: 0.5699\n",
      "Epoch 420/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 0.5695\n",
      "Epoch 421/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5569 - val_loss: 0.5692\n",
      "Epoch 422/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5693\n",
      "Epoch 423/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5762 - val_loss: 0.5690\n",
      "Epoch 424/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5524 - val_loss: 0.5688\n",
      "Epoch 425/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5932 - val_loss: 0.5690\n",
      "Epoch 426/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5540 - val_loss: 0.5684\n",
      "Epoch 427/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5390 - val_loss: 0.5688\n",
      "Epoch 428/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5651 - val_loss: 0.5684\n",
      "Epoch 429/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5608 - val_loss: 0.5680\n",
      "Epoch 430/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5681\n",
      "Epoch 431/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5678\n",
      "Epoch 432/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5639 - val_loss: 0.5680\n",
      "Epoch 433/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 0.5673\n",
      "Epoch 434/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5672 - val_loss: 0.5675\n",
      "Epoch 435/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5726 - val_loss: 0.5672\n",
      "Epoch 436/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.5668\n",
      "Epoch 437/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5702 - val_loss: 0.5666\n",
      "Epoch 438/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5663\n",
      "Epoch 439/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5621 - val_loss: 0.5664\n",
      "Epoch 440/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5465 - val_loss: 0.5665\n",
      "Epoch 441/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 0.5661\n",
      "Epoch 442/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5783 - val_loss: 0.5660\n",
      "Epoch 443/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5584 - val_loss: 0.5658\n",
      "Epoch 444/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.5668\n",
      "Epoch 445/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 0.5656\n",
      "Epoch 446/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5658\n",
      "Epoch 447/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5695 - val_loss: 0.5655\n",
      "Epoch 448/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5663 - val_loss: 0.5651\n",
      "Epoch 449/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5547 - val_loss: 0.5656\n",
      "Epoch 450/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5650\n",
      "Epoch 451/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5647\n",
      "Epoch 452/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.5642\n",
      "Epoch 453/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5569 - val_loss: 0.5647\n",
      "Epoch 454/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5499 - val_loss: 0.5640\n",
      "Epoch 455/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 0.5643\n",
      "Epoch 456/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5641\n",
      "Epoch 457/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5672 - val_loss: 0.5636\n",
      "Epoch 458/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 0.5634\n",
      "Epoch 459/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5580 - val_loss: 0.5640\n",
      "Epoch 460/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5783 - val_loss: 0.5636\n",
      "Epoch 461/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5674 - val_loss: 0.5631\n",
      "Epoch 462/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5532 - val_loss: 0.5630\n",
      "Epoch 463/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5703 - val_loss: 0.5630\n",
      "Epoch 464/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5747 - val_loss: 0.5632\n",
      "Epoch 465/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 0.5626\n",
      "Epoch 466/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5531 - val_loss: 0.5625\n",
      "Epoch 467/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5565 - val_loss: 0.5622\n",
      "Epoch 468/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5622\n",
      "Epoch 469/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5747 - val_loss: 0.5623\n",
      "Epoch 470/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5694 - val_loss: 0.5616\n",
      "Epoch 471/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5523 - val_loss: 0.5617\n",
      "Epoch 472/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5613 - val_loss: 0.5618\n",
      "Epoch 473/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5525 - val_loss: 0.5617\n",
      "Epoch 474/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5501 - val_loss: 0.5614\n",
      "Epoch 475/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5507 - val_loss: 0.5611\n",
      "Epoch 476/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5612 - val_loss: 0.5607\n",
      "Epoch 477/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.5608\n",
      "Epoch 478/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.5606\n",
      "Epoch 479/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 0.5606\n",
      "Epoch 480/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.5610\n",
      "Epoch 481/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5605\n",
      "Epoch 482/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.5601\n",
      "Epoch 483/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5759 - val_loss: 0.5599\n",
      "Epoch 484/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5539 - val_loss: 0.5598\n",
      "Epoch 485/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 0.5602\n",
      "Epoch 486/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5595\n",
      "Epoch 487/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5458 - val_loss: 0.5593\n",
      "Epoch 488/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5590\n",
      "Epoch 489/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 0.5591\n",
      "Epoch 490/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5631 - val_loss: 0.5587\n",
      "Epoch 491/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5585\n",
      "Epoch 492/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 0.5587\n",
      "Epoch 493/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5585\n",
      "Epoch 494/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5626 - val_loss: 0.5586\n",
      "Epoch 495/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5557 - val_loss: 0.5582\n",
      "Epoch 496/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5492 - val_loss: 0.5580\n",
      "Epoch 497/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.5578\n",
      "Epoch 498/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 0.5576\n",
      "Epoch 499/501\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5586 - val_loss: 0.5578\n",
      "Epoch 500/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5559 - val_loss: 0.5578\n",
      "Epoch 501/501\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5621 - val_loss: 0.5575\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_arr,y_arr, epochs=n_iters, batch_size=batch_size,\n",
    "                    validation_data = (X_test_arr,y_test_arr),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e18588",
   "metadata": {
    "papermill": {
     "duration": 0.248966,
     "end_time": "2021-08-01T08:07:02.166000",
     "exception": false,
     "start_time": "2021-08-01T08:07:01.917034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predicting through keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf833e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T08:07:02.670480Z",
     "iopub.status.busy": "2021-08-01T08:07:02.669543Z",
     "iopub.status.idle": "2021-08-01T08:07:02.778731Z",
     "shell.execute_reply": "2021-08-01T08:07:02.779519Z",
     "shell.execute_reply.started": "2021-08-01T07:54:17.684176Z"
    },
    "papermill": {
     "duration": 0.362708,
     "end_time": "2021-08-01T08:07:02.779718",
     "exception": false,
     "start_time": "2021-08-01T08:07:02.417010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------- Evaluation ----------------------#\n",
      "ROC AUC of test set : 0.7473958333333333\n",
      "Accuracy of test set : 0.77\n"
     ]
    }
   ],
   "source": [
    "keras_pred = model.predict(X_test_arr)\n",
    "keras_pred = np.where(keras_pred>0.5,1,0)\n",
    "\n",
    "#print(np.unique(keras_pred))\n",
    "print('#--------------------- Evaluation ----------------------#')\n",
    "#Evaluation of the predictions\n",
    "print('ROC AUC of test set :',roc_auc_score(y_test_arr.ravel(),keras_pred.ravel()))\n",
    "print('Accuracy of test set :',accuracy_score(y_test_arr.ravel(),keras_pred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cebc83",
   "metadata": {
    "papermill": {
     "duration": 0.247202,
     "end_time": "2021-08-01T08:07:03.276100",
     "exception": false,
     "start_time": "2021-08-01T08:07:03.028898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Insights : The manual implementation of ANN is giving very similar predictions (better by ~7% !!) as that to the Keras counterparts, indicating the implementation is correct and comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73394e",
   "metadata": {
    "papermill": {
     "duration": 0.248076,
     "end_time": "2021-08-01T08:07:03.772080",
     "exception": false,
     "start_time": "2021-08-01T08:07:03.524004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d9050",
   "metadata": {
    "papermill": {
     "duration": 0.247304,
     "end_time": "2021-08-01T08:07:04.268067",
     "exception": false,
     "start_time": "2021-08-01T08:07:04.020763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.840689,
   "end_time": "2021-08-01T08:07:06.624303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-01T08:06:02.783614",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
